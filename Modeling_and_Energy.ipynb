{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors Modeling and Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides the code to:\n",
    "\n",
    "1. Model with Alphafold2-Multimer the desired target given a fasta file\n",
    "\n",
    "2. Relax the models created Alphafold2-Multimer\n",
    "\n",
    "3. Extract, convert and create a pseudo-log.txt from AF3 models\n",
    "\n",
    "4. Calculate the bind Energy of Alphafold2-Multimer and AF3 models\n",
    "\n",
    "There are two alternative codes A and B. A is for Local calculations and B for calculations using MN5 (Cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings of the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File manegement\n",
    "import os, zipfile \n",
    "import re \n",
    "import shutil\n",
    "\n",
    "# Data manegement\n",
    "import pandas as pd # used to manage dataframes\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from Bio import PDB\n",
    "from Bio.PDB import MMCIFParser, PDBIO, DSSP, NeighborSearch,Superimposer,PDBParser\n",
    "from Bio.Align import PairwiseAligner\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import warnings\n",
    "# Subprocess to calling bash\n",
    "import subprocess # used to call bash and running external programs like pydock4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target election and p_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you choose:\n",
    "\n",
    "- The target.\n",
    "\n",
    "- The template to copy the existing scripts.\n",
    "\n",
    "- The folder where we collect the fastas for modeling.\n",
    "\n",
    "- The test settings (Predictors or Scorers) and whether it is local or on MareNostrum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del directorio principal\n",
    "dir_capri=\"\"\n",
    "\n",
    "#Plantilla  \n",
    "nplantilla=\"T901\" \n",
    "\n",
    "# Seleccion de target\n",
    "target=\"T902\" \n",
    "\n",
    "### Settings\n",
    "p_type= \"Predictors\"\n",
    "\n",
    "# Definicion de directorios\n",
    "fasta_dir=os.path.join(dir_capri,\"fastas\") # Where the fastas are gatherer\n",
    "dir_target=os.path.join(dir_capri,target) # Directory of the complete target\n",
    "dir_template=os.path.join(dir_capri,nplantilla)# Template with all the necesary scripts\n",
    "dir_complex=f\"{dir_target}/Predictors/AF_MODELS/COMPLEX\" # This directory is needed to execute several codes\n",
    "\n",
    "# Definicion de archivos fasta\n",
    "original_fasta=f\"{fasta_dir}/{target}.fasta\"\n",
    "fasta=f\"{dir_complex}/{target}.fasta\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the target folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente bloque copiamos la plantilla, con un nuevo nombre. Despu√©s el fasta de la carpeta donde se almacenan (fasta_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure that the fasta file is in fasta_dir\n",
    "\n",
    "# Create the target folder with all the scripts from the template if the fasta exists\n",
    "if not os.path.exists(dir_target):\n",
    "    shutil.copytree(dir_template, dir_target)\n",
    "    print(f\"Folder created: {dir_target}\")\n",
    "else:\n",
    "    print(\"The folder already exists\")\n",
    "\n",
    "if os.path.exists(original_fasta):\n",
    "    shutil.copy2(original_fasta, fasta)\n",
    "    print(f\"File {original_fasta} copied to {fasta}\") \n",
    "else: \n",
    "    print(f\"The fasta doesn't exist in {original_fasta} or {dir_target} doesn't exist\")\n",
    "    assert False, f\"Place the fasta file in {fasta_dir}\"\n",
    "\n",
    "# Ensure that the file is inside the target folder\n",
    "# Remember that the original_fasta needs to be in fasta_dir to copy correctly\n",
    "\n",
    "if os.path.exists(fasta):\n",
    "    print(\"Existing file\", fasta)\n",
    "else: \n",
    "    print(\"File does not exist\", fasta)\n",
    "    assert False, f\"Warning, the file was not copied correctly, fasta is not in {fasta}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Template copied, now go to local or to server\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Local Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Alphafold2-Multimer modeling (LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if local:\n",
    "    print(\"local\")\n",
    "    subprocess.run(['bash',\"conda activate Alphafold2\\n\",\"bash ./script_calculo_local.sh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Relaxation (LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "def relaxation(dir_complex, target):\n",
    "\n",
    "\n",
    "    greasy = 'greasy'\n",
    "    colabfold_relax = 'colabfold_relax'\n",
    "    \n",
    "    # Creation of each command of relaxation to pass to shell script\n",
    "    os.chdir(dir_complex)\n",
    "    print(os.getcwd())\n",
    "    os.chdir('..')\n",
    "    complex_dir = os.getcwd()\n",
    "    fasta_files = glob.glob(os.path.join(complex_dir, '*.fasta'))\n",
    "    for fasta_file in fasta_files:\n",
    "        base_name = os.path.basename(fasta_file).replace('.fasta', '')\n",
    "        cmd_file = os.path.join(complex_dir, f'{base_name}.colabfold.relax.cmd')\n",
    "        \n",
    "        with open(cmd_file, 'w') as cmd_fp:\n",
    "            cmd_fp.write(f'#!/bin/bash\\n{greasy} {base_name}.colabfold.relax.greasy\\n')\n",
    "        \n",
    "        pdb_files = glob.glob(os.path.join(complex_dir, 'T*_*.r*.pdb'))\n",
    "        \n",
    "        with open(cmd_file, 'a') as cmd_fp:\n",
    "            for pdb_file in pdb_files:\n",
    "                cmd_fp.write(f\"{colabfold_relax} --max-iterations 2000 --tolerance 2.39 --stiffness 10.0 --max-outer-iterations 3 --use-gpu {pdb_file} {pdb_file.replace('/COMPLEX', '')}\\n\")\n",
    "\n",
    "    # Relaxation execution\n",
    "    os.environ['GREASY_NWORKERS'] = '2' # change if you need more, keep in mind that more workers could lead to abortion of execution\n",
    "    os.chdir(complex_dir)\n",
    "    file_path=f'{os.getcwd()}/COMPLEX/{target}.colabfold.relax.cmd'# Check if the file is correcly created\n",
    "    return file_path\n",
    "   \n",
    "file_path=relaxation(dir_complex, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that CMD is correctly generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd= open(file_path)\n",
    "content= cmd.read()\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Look at the script, comment this line if you don't want to pause the execution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of the relaxation\n",
    "subprocess.run(['bash', file_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AF3 models processing (LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of Alphafold3 Models\n",
    "1. Decompression\n",
    "\n",
    "2. Conversion to PDB\n",
    "\n",
    "3. Extraction of data from the JSON: iptm and ptm to save the model confidence in an artificial log.txt, similar to the one from Alphafold Multimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 4px solid; background-color:#FF6347; color: #FFFFFF; padding: 10px;\">\n",
    "    <strong style=\"color: #FF6347;\">Nota:</strong>\n",
    "    <span style=\"color: #000000;\">WARNING: rebember to download the AF3 models and have them in the dir_complex</span>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress the AlphaFold3 Job.\n",
    "def descomprimir_archivo(zip_path, directorio_destino):\n",
    "\n",
    "    # Ensure that the destination directory exists; if not, create it\n",
    "    if not os.path.exists(directorio_destino):\n",
    "        os.makedirs(directorio_destino)\n",
    "\n",
    "    # Open the ZIP file in read mode\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all files into the specified directory\n",
    "        zip_ref.extractall(directorio_destino)\n",
    "\n",
    "patron = r'(.*(\\d+)\\.zip$)'\n",
    "patron_CIF = r'(.*(\\d+)\\.cif$)'\n",
    "\n",
    "zipfiles = [os.path.abspath(os.path.join(dir_complex, archivo)) for archivo in os.listdir(dir_complex) if re.match(patron, archivo)]\n",
    "print(zipfiles)\n",
    "for zipfille in zipfiles:\n",
    "    print(zipfille)\n",
    "    descomprimir_archivo(zipfille, zipfille.rstrip('.zip'))\n",
    "\n",
    "CIF_files = [os.path.abspath(os.path.join(dir_complex, archivo)) for archivo in os.listdir(zipfille.rstrip('.zip')) if re.match(patron_CIF, archivo)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Conversion to PDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert CIF files to PDB files of the AplhaFold3 Job.\n",
    "def convert_cif_to_pdb(cif_file, pdb_file):\n",
    "    \"\"\"\n",
    "    Convert a CIF file to a PDB file using Biopython.\n",
    "\n",
    "    Parameters:\n",
    "    cif_file (str): Path to the input CIF file.\n",
    "    pdb_file (str): Path to the output PDB file.\n",
    "    \"\"\"\n",
    "    parser = MMCIFParser()\n",
    "    structure = parser.get_structure('ID', cif_file)\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(pdb_file)\n",
    "\n",
    "patron_CIF = r'(.*(\\d+)\\.cif$)'\n",
    "for zipfille in zipfiles:\n",
    "    CIF_files = [os.path.join(zipfille.rstrip('.zip'),archivo) for archivo in os.listdir(zipfille.rstrip('.zip')) if re.match(patron_CIF, archivo)]\n",
    "    #print(CIF_files)\n",
    "    for CIF_file in CIF_files:\n",
    "        #print(CIF_file)\n",
    "        convert_cif_to_pdb(CIF_file, CIF_file.replace('.cif','.pdb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 JSON data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Json\n",
    "import os\n",
    "import json\n",
    "\n",
    "def leer_json_extract_vars(directorio, claves):\n",
    "    \"\"\"\n",
    "    Lee archivos JSON en un directorio espec√≠fico y extrae las variables especificadas.\n",
    "    \n",
    "    Par√°metros:\n",
    "    directorio (str): Ruta al directorio que contiene los archivos JSON.\n",
    "    claves (list): Lista de claves a extraer de los archivos JSON.\n",
    "    \n",
    "    Retorna:\n",
    "    dict: Diccionario con nombres de archivo y sus variables extra√≠das.\n",
    "    \"\"\"\n",
    "    resultados = {}  # Diccionario para almacenar los resultados\n",
    "\n",
    "    # Recorrer todos los archivos en el directorio\n",
    "    pattern_json = re.compile(r\"summary_confidences_\\w\\.json$\")\n",
    "    for archivo in os.listdir(directorio):\n",
    "        if pattern_json.search(archivo):  # Asegurarse de que es un archivo JSON\n",
    "            ruta_completa = os.path.join(directorio, archivo)\n",
    "            with open(ruta_completa, 'r') as f:\n",
    "                data = json.load(f)  # Cargar el contenido JSON\n",
    "                # Extraer las variables especificadas\n",
    "                valores_extraidos = {clave: data.get(clave, None) for clave in claves}\n",
    "                \n",
    "                # Almacenar los resultados\n",
    "                resultados[archivo] = valores_extraidos\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Usar la funci√≥n\n",
    "claves_a_extraer = ['ptm', 'iptm']  # A√±adir aqu√≠ cualquier clave que necesites\n",
    "for zipfille in zipfiles:\n",
    "    log_folder = zipfille.rstrip('.zip')\n",
    "    resultados = leer_json_extract_vars(zipfille.rstrip('.zip'), claves_a_extraer)\n",
    "    with open(os.path.join(log_folder,'log.txt'), 'w') as file:\n",
    "        for archivo, vars in resultados.items():\n",
    "            # Formatear nombre del archivo y eliminar partes no deseadas\n",
    "            nombre_archivo_formateado = archivo.replace('summary_confidences', 'model').rstrip('.json')\n",
    "            # Crear una cadena de texto con los pares clave=valor\n",
    "            vars_text = ' '.join([f\"{key.replace('tm','TM')}={value}\" for key, value in vars.items()])\n",
    "            file.write(f\"{nombre_archivo_formateado} {vars_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.run(['bash',f\"scp -r fold_t282_1 bsc054620@glogin1.bsc.es:/gpfs/projects/bsc54/Capri/{target}/Predictors/AF_MODELS/COMPLEX/\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. pyDock Bind Energy (Local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ligand and receptor asignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multip=True\n",
    "if multip:\n",
    "    script_content=\"\"\"\n",
    " #!/bin/bash\n",
    "        #rm Greasy_BindE_mul_ligs.txt;\n",
    "        CHAINS_LIG_VALUES=(\n",
    "            \"B,C,D A\"\n",
    "            \"A,C,D B\"\n",
    "            \"A,B,D C\"\n",
    "            \"A,B,C D\"\n",
    "            \"A E,G\"\n",
    "            \"B I,K\"\n",
    "            \"C F,H\"\n",
    "            \"D J,L\"\n",
    "        )\n",
    "\n",
    "        for h in $(ls -d  T*cola*/ fold*/);do\n",
    "                echo $h;\n",
    "                cd $h;\n",
    "                for j in *.pdb;do\n",
    "                        echo $j\n",
    "                        for value in \"${CHAINS_LIG_VALUES[@]}\"; do\n",
    "                    CHAINS_LIG=\"$value\"\n",
    "                    LIG=echo $CHAINS_LIG | cut -d\" \" -f 2\n",
    "                            REC=echo $CHAINS_LIG | cut -d\" \" -f 1\n",
    "                            CH=${LIG/,}\n",
    "                            echo -e \"[receptor]\\npdb         = $j\\nmol         = $REC\\nnewmol      = $REC\" > ${j/.pdb}_LIG_${CH}.ini;\n",
    "                            echo -e \"[ligand]\\npdb         = $j\\nmol         = $LIG\\nnewmol      = $LIG\" >>  ${j/.pdb}_LIG_${CH}.ini;\n",
    "                            #echo -e \"[reference]\\npdb         = ranked_0_REF.pdb\\nrecmol      = $REC\\nligmol      = $LIG\\nnewrecmol   = $REC\\nnewligmol   = $LIG\\n\" >> ${j/.pdb}_LIG_${CH}.ini;\n",
    "                            echo \"cd ${h}; timeout 5m pydock4 ${j/.pdb}_LIG_${CH} bindEy;cd -\" >> ../Greasy_BindE_mul_ligs.txt;\n",
    "                done\n",
    "                done\n",
    "                cd -;\n",
    "        done\n",
    "        \"\"\"\n",
    "\n",
    "    comando=f\"cd {dir_complex}\\n bash {script_content}\"\n",
    "    #subprocess.run(['bash',f\"{comando}\"])\n",
    "else:\n",
    "    comando=f\"cd {dir_complex}\\n bash ./ini_creator_bindE.sh\"\n",
    "    subprocess.run(['bash',f\"{comando}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bind Energy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comando=f\"cd {dir_complex}\\n bash ./script_calculo_local.sh\"\n",
    "if local:\n",
    "    subprocess.run(['bash', f\"{comando}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Energy summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comando=f\"cd {dir_complex}\\n bash ./sum_ene_multy_bindEy_new.sh\"\n",
    "if local:\n",
    "    subprocess.run(['bash',f\"{comando}\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
