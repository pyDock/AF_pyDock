{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5bdfc94",
   "metadata": {},
   "source": [
    "# Alphafold models analysis main program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317f506",
   "metadata": {},
   "source": [
    "##  Description of the materials and program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd66594",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7624a2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "   \n",
    "This jupyter notebook is created to perform a analysis of complexes generated by different versions of Alphafold. There are main 4 versions of AlphaFold available:\n",
    "\n",
    "- AlphaFold2-Multimer v1 (v1).\n",
    "\n",
    "- AlphaFold2-Multimer v2 (v2).\n",
    "\n",
    "- AlphaFold2-Multimer v3 (v3).\n",
    "\n",
    "- AF3.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa11ba4",
   "metadata": {},
   "source": [
    "### Description of the files and folders of AlphaFold2-Multimer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25140c",
   "metadata": {},
   "source": [
    "#### Complex folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caec483",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "The folder in which the rest files are stored are named by the complex, composed by the name of the cristal in the PDB bank followed by the chains used to do the complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d2f0f",
   "metadata": {},
   "source": [
    "#### PDBS files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526df74",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1; text-align: justify;\">\n",
    "\n",
    "Indicates the information of the protein structure. The names of the PDBs generated by Aplhafold Multimer are composed by: complex,state,rank,version of Alphafold, model and recycle (except cristals,\"ranked\" pdbs and Seed_0 pdbs).<br><br>\n",
    "\n",
    "- Complex: the name of the complex registered in the PDB bank, it  is composed by letters and numbers.<br><br>\n",
    "- States:\n",
    "  - unrelaxed: are crude structures provided by Alphafold in which it does it iterative proccess .\n",
    "  \n",
    "  - relaxed: The last structure recycled relaxed using AMBER  force field in openMM. <br><br>\n",
    "\n",
    "- Version: indicates which version of AlphaFold .<br><br>\n",
    "\n",
    "\n",
    "- Model:\n",
    "\n",
    "  - Models in Alphafold2: generates five predictions from the same seed, are named as \"model_\" followed by a number.\n",
    "  \n",
    "\n",
    "    - \"ranked_\" folled by a number: indicates in which position in the rank are the relaxed models according to the scores that alphafold assigns. Their name is entirely \"ranked\" it has no more data in it.\n",
    "\n",
    "\n",
    "    - \"pred_\" followed by a number: identifies a model generated by the same seed, but with minor differences.<br><br>\n",
    "    \n",
    "  \n",
    "  - Model in the versions of AM (v1,v2,v3,v3_short): the models are generate models 5 model from differents seeds and then it iterates the resolution of the structure until the tol variable surpass a threshold in which alphafold stop modeling ot reaches the recycle of 20.<br><br>\n",
    "\n",
    "- Recycle: only for non-Alphafold2 predictions (at te moment).\n",
    "  \n",
    "  - \"r_\" followed by number : indicates the recycle of the model.\n",
    "\n",
    "\n",
    "  - \"Seed_0\": is the same from recycle 20 that will be relaxed.<br><br>\n",
    "  \n",
    "- Rank folllowed by a number : it indicates which model of the five generated is best according to the highest score obtained in the last recyle, only in Alphafold2.<br><br>\n",
    "\n",
    "\n",
    "- Examples of names:\n",
    "\n",
    "\n",
    "  - unrelaxed_rank_001_alphafold2_multimer_v2_model_4_seed_000_r9.pdb (standart name in AM versions).\n",
    "\n",
    "\n",
    "  - relaxed_model_4_multimer_v2_pred_1.pdb (standart name in Alphafold2 versions).\n",
    "\n",
    "\n",
    "  - 3BT1.pdb (crystal).\n",
    "\n",
    "\n",
    "  - ranked_0.pdb (relaxed and ranked in Alphafold2).\n",
    "\n",
    "  \n",
    "  - unrelaxed_rank_001_alphafold2_multimer_v3_model_2_seed_000_r0 ( Seed_0 example).\n",
    "\n",
    "   \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d3f39",
   "metadata": {},
   "source": [
    "#### Log.txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a44d6",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1; text-align: justify;\">\n",
    "   \n",
    "It gathers infromation about the execution of alphafold, the most relevant information is:\n",
    "\n",
    "- Timestamps: The file starts with timestamps indicating when each event occurred. These timestamps are in the format \"YYYY-MM-DD HH:MM:SS,sss\" (Year, Month, Day, Hour, Minute, Second, Milliseconds).\n",
    "\n",
    "- Information about the software: The first few entries provide information about the software version (ColabFold 1.5.2).\n",
    "\n",
    "- Recycle iterations: The log then proceeds to provide information about the iterative process of protein structure prediction. It mentions recycling and various metrics such as \"pLDDT,\" \"pTM,\" \"ipTM,\" and \"tol\" for each recycle step.\n",
    "\n",
    "- Model ranking: The final section ranks the models based on the \"multimer\" metric, and it mentions the relaxation times for each model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5a48f",
   "metadata": {},
   "source": [
    "### Description of the files and folders of Alphafold 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abc518",
   "metadata": {},
   "source": [
    "#### JSON\n",
    "\n",
    "Full-data JSON: It gives detailed information about each residue\n",
    "\n",
    "Job_request JSOn: is the job submited to AF3 server. It contains the name of the job (usually the modeled complex), the seed designated (random) and the sequences of the desired molecules. If this job is uploaded to AF3 server y reproduces the same results.\n",
    "\n",
    "Summary confidence: it gives information about the overall quality of the structure. It is mainly composed by:\n",
    "\n",
    " - \"fraction_disordered\": the disorded regions are defined  in the supplementary work of AF3\n",
    " - \"has_clash\": indicates the proportion of clases\n",
    " - \"iptm\": the interface of TM scored, is calculated with the same procedure as in AF2-Multimer\n",
    " - \"num_recycles\": number of recycles done by the pairformer, for more information (https://elanapearl.github.io/blog/2024/the-illustrated-alphafold/)\n",
    " - \"ptm\": proximated TM scored, is calculated with the same procedure as in AF2-Multimer\n",
    " - \"ranking_score\": new score of AF3 which includes iptm, ptm, fraction_disoredred and clases to acoid hallucinations: 0.8 · ipTM + 0.2 · pTM + 0.5 · disorder − 100 · has_clash. \n",
    "\n",
    "cif models: similar to PDB, you can use programs like Chimera X and Pymol to look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328da18",
   "metadata": {},
   "source": [
    "### Description of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab0867",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "   \n",
    "The analysis in the main program is divided in 6 sections:\n",
    "\n",
    "1. Libraries and initial values: It loads the libraries are needed. \n",
    "2. Paths and selected molecules: Selection of the target and the p_type\n",
    "3. Contruction of the dataframes: In this sections we extract the information of ene files and log txt in to dataframes\n",
    "4. Final Fusion and adjustments: \n",
    "5. Ranking: we calculate\n",
    "\n",
    "There are two classes of folders. Ones have the pdb from Alphafold2 and the other are obtained from AlphaFold-multimers. The difference between them is how the information about the model confidence is stored, the ones from Alphafold2 have their model confidence stored in json archives and the ones from AM have in the log.txt. This implies a different aproach of gathering this data.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a7756",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158cdc73",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9f1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File manegement\n",
    "import os, zipfile \n",
    "import re \n",
    "import shutil\n",
    "\n",
    "# Data manegement\n",
    "import pandas as pd # used to manage dataframes\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from Bio import PDB\n",
    "from Bio.PDB import MMCIFParser, PDBIO, DSSP, NeighborSearch, Superimposer, PDBParser\n",
    "from Bio.Align import PairwiseAligner\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import warnings\n",
    "\n",
    "# Subprocess to calling bash\n",
    "import subprocess # used to call bash and running external programs like pydock4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a4adc",
   "metadata": {},
   "source": [
    "### 2. Paths and selected molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf408b0",
   "metadata": {},
   "source": [
    "Selection of path and molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64440d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luis/CAPRI_R57/T309/Predictors/AF_MODELS/COMPLEX/\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "Target_name=\"T309\"\n",
    "directorio=f\"/home/luis/CAPRI_R57/{Target_name}/Predictors/AF_MODELS/COMPLEX/\"\n",
    "directorio_csv= f\"/home/luis/CAPRI_R57/{Target_name}/Predictors/AF_MODELS/COMPLEX/\" # This is the the directory of the folder that will gather the outputs\n",
    "to_send_dir=f\"/home/luis/CAPRI_R57/{Target_name}/Predictors/To_send/\"\n",
    "to_send_csv =f\"/home/luis/CAPRI_R57/{Target_name}/Predictors/To_send/{Target_name}_predictor_to_send.ene\"\n",
    "\n",
    "# Target_name=\"T254\"\n",
    "# directorio=\"/home/luis/CAPRI_R57/T254/Predictors/Superposition_models_T255_new/\"\n",
    "# directorio_csv= \"//home/luis/CAPRI_R57/T254/Predictors/Superposition_models_T255_new/\"# This is the the directory of the folder that will gather the outputs\n",
    "\n",
    "# Target_name=\"T272\"\n",
    "# directorio=\"/home/luis/CAPRI_R57/T272/Predictors/SUPERPOSITION_MODELS/\"\n",
    "# directorio_csv= \"/home/luis/CAPRI_R57/T272/Predictors/SUPERPOSITION_MODELS/\"# This is the the directory of the folder that will gather the outputs\n",
    "\n",
    "#Clustering\n",
    "#receptor_mol,ligand_mol =[\"A\",\"B\"] #T236\n",
    "#receptor_mol,ligand_mol =[\"A,B\",\"C\"] #T238\n",
    "#receptor_mol,ligand_mol =[\"A,B,C,D\"],[\"E\"] #240\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T242\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T244\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T248\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T248\n",
    "#receptor_mol,ligand_mol =[\"A\",\"B\"],[\"C\"] #T250/T252\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T254/T255 por simertria solo cogemos dos cadenas\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T262 por simertria solo cogemos dos cadenas\n",
    "# receptor_mol,ligand_mol =[\"B,C\"],[\"A\"] #T266 Antibody\n",
    "# receptor_mol,ligand_mol =[\"B\"],[\"I\",\"K\"] #T264-T265 Protein_DNA\n",
    "# receptor_mol,ligand_mol =[\"A\",\"C,E\"],[\"B\",\"K,F\"] #T280\n",
    "#receptor_mol,ligand_mol =[\"B,C\"],[\"A\"] #T284\n",
    "#receptor_mol,ligand_mol =[\"B,C\"],[\"A\"], [\"A,C\"],[\"B\"], [\"B,A\"],[\"C\"] # T288\n",
    "#receptor_mol,ligand_mol =# T290\n",
    "\n",
    "receptor_mol,ligand_mol =[\"A,B,C\"],[\"D,E,F\"] #T290\n",
    "#receptor_mol,ligand_mol =[\"A,B,C\"],[\"D,E,F,G,H,I\"] #T292\n",
    "\n",
    "#receptor_mol,ligand_mol =[\"B,C\"],[\"A\"] #T266 Antibody\n",
    "#receptor_mol,ligand_mol =[\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],[\"A\"] #T272 Antibody\n",
    "\n",
    "\n",
    "print(directorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fb981",
   "metadata": {},
   "source": [
    "Looking at available pdbs, to see if the modeling process have be done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directorio_csv):\n",
    "    os.makedirs(directorio_csv)\n",
    "\n",
    "# Folders of all models\n",
    "carpetas = [nombre for nombre in os.listdir(directorio) if os.path.isdir(os.path.join(directorio, nombre))]\n",
    "\n",
    "#PDB files of the folders and the way we will \n",
    "archivos_pdb=[]\n",
    "patron = r'(.*(\\d+)\\.pdb$)'\n",
    "#patron = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.pdb' # T272\n",
    "datos_carpeta={}\n",
    "for carpeta in carpetas:  \n",
    "        patron =\"(\"+carpeta[0:4]+ \".pdb)|\" +patron\n",
    "        direccion = directorio + \"/\" + carpeta + \"/\"\n",
    "        pdbs=[os.path.abspath(os.path.join(direccion, archivo)) for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "        datos_carpeta={**datos_carpeta,**{carpeta:len(pdbs)}}\n",
    "        archivos_pdb.extend(pdbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6713641",
   "metadata": {},
   "outputs": [],
   "source": [
    "carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3137b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_carpeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576816fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numero_pdbs_by_dir(directorio):  \n",
    "    x=1\n",
    "    n_archivos=0\n",
    "    for carpeta in carpetas:\n",
    "        # Accedemos a cada una de ellas y ponemos en un documento lista la dirección de cada uno de los .pdb\n",
    "        direccion = directorio + \"/\" + carpeta + \"/\"\n",
    "        archivos_pdb = [archivo for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "        print(x,carpeta,len(archivos_pdb))\n",
    "        n_archivos=n_archivos+len(archivos_pdb) \n",
    "        x=x+1\n",
    "    return (n_archivos)\n",
    "print(numero_pdbs_by_dir(directorio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ffcd45",
   "metadata": {},
   "source": [
    "### 3. Data Frame creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77541622",
   "metadata": {},
   "source": [
    "#### 3.1 Description of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f7000",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "\n",
    "The dataframe constructed from all the following process is:\n",
    "\t\n",
    "- **Ele**: Electrostatic energy of the complex. Measures the interaction between electric charges within the complex.\n",
    "- **Desolv**: Desolvation energy. Represents the energetic cost associated with desolvating individual molecules to form the complex.\n",
    "- **VDW**: Van der Waals energy. Measures the attractive and repulsive interactions between atoms that are not chemically bonded.\n",
    "- **Total**: Total energy of the complex. Sum of all energetic contributions (Electrostatic + Desolvation + 0.1 Van der Waals).\n",
    "- **Name**: Name of the object or element. Used to identify and merge data from different datasets.\n",
    "- **PATH**: File path associated with the object. Stores the locations of the files corresponding to each object for additional input/output operations.\n",
    "- **Complex**: Name or identifier of the studied complex.\n",
    "- **State**: State of the complex (e.g., native, mutated, etc.).\n",
    "- **Model**: Specific model used in the analysis.\n",
    "- **Rank**: Ranking of the model or complex based on a specific criterion.\n",
    "- **Version**: Version of the model or software used in the analysis.\n",
    "- **Recycle**: Number of times the model has been recycled or reused in iterations.\n",
    "- **Seed**: Seed value used by AlphaFold2.\n",
    "- **Unstructured_count**: Number of unstructured regions in the complex.\n",
    "- **Max_unstructured_region**: Size of the largest unstructured region.\n",
    "- **Total_clashes**: Total number of atomic clashes within the complex.\n",
    "- **Clashes_chain_A**: Number of clashes in chain A.\n",
    "- **Clashes_chain_B**: Number of clashes in chain B. _There may be more chains._\n",
    "- **Low_B_factors_chain_A**: Percentage of residues with pLDDT below 50 in chain A.\n",
    "- **Low_B_factors_chain_B**: Percentage of residues with pLDDT below 50 in chain B. _There may be more chains._\n",
    "- **Knots**: Number of knots present in the structure.\n",
    "- **pLDDT**: Predicted Local Distance Difference Test. Measures the quality of the local structural prediction.\n",
    "- **pTM**: Predicted Template Modeling. Measures the quality of the global structural prediction based on template modeling.\n",
    "- **ipTM**: Interface Predicted Template Modeling. Measures the quality of the structural prediction at interfaces.\n",
    "- **tol**: Tolerance of the model or simulation.\n",
    "- **Model_confidence**: Confidence in the predictive model. Calculated as ipTM\\*0.8 + pTM\\*0.3.\n",
    "- **Total2**: Unweighted total energy from pyDock (Electrostatic + Desolvation + Van der Waals).\n",
    "- **MCZ-Score**: Model Confidence Z-score.\n",
    "- **PLDDTZ-Score**: pLDDT Z-score.\n",
    "- **TEZ-Score**: Z-score calculated from Total.\n",
    "- **TE2Z-Score**: Z-score calculated from Total2.\n",
    "- **Sum_Z**: Sum of the Z-scores for Model Confidence and Total.\n",
    "- **Sum2_Z**: Sum of the Z-scores for Model Confidence and Total2.\n",
    "- **Z-PLT**: Sum of the Z-scores for pLDDT and Total.\n",
    "- **Z-PLT2**: Sum of the Z-scores for pLDDT and Total2.\n",
    "- **Ranking_Z**: Ranking based on Sum_Z.\n",
    "- **Ranking2_Z**: Ranking based on Sum2_Z.\n",
    "- **Ranking_PLT**: Ranking based on the Z-PLT criterion.\n",
    "- **Ranking_PLT2**: Ranking based on the Z-PLT2 criterion.\n",
    "- **Diferencia_R2_Z**: Difference between the current ranking and the next in the Ranking2_Z column. Indicates the cluster size.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b5c96a1",
   "metadata": {},
   "source": [
    "#### 3.2 Bind energy dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ac929",
   "metadata": {},
   "source": [
    "##### 3.2.1 Extraction of the information from .ene tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dataframe that will gather all the results\n",
    "total_df=pd.DataFrame()\n",
    "resultado_df = pd.DataFrame()\n",
    "extension_final = len(\".ene\")\n",
    "patron = r\".*\\d\\.ene$\"\n",
    "\n",
    "# Iteration of each folder, we extract all the names of the .ene inside \n",
    "for carpeta in carpetas:\n",
    "    direccion = os.path.join(directorio, carpeta )\n",
    "    archivos_ene = [archivo for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "    resultado_df = pd.DataFrame() # the dataframe with all the ene data of the folder, important to distinguish between Af3 and Af2-multimer\n",
    "    \n",
    "    # Interation of each .ene and extracting their information in to a single dataframe\n",
    "    for archivo in archivos_ene:\n",
    "        print(os.path.join(direccion, archivo))\n",
    "        tabla = []\n",
    "        df = pd.read_csv(os.path.join(direccion, archivo), sep='\\s+', skiprows=[1])\n",
    "        df[\"Name\"] = archivo[:-extension_final]\n",
    "        df[\"PATH\"] = os.path.join(direccion, archivo).rstrip(\".ene\")+\".pdb\"\n",
    "        print(df)\n",
    "        resultado_df = pd.concat([resultado_df, df], ignore_index=True)\n",
    "\n",
    "    # We add the information of the complex depending if the folder is from AF3\n",
    "    if carpeta.startswith('fold'):\n",
    "        resultado_df[\"Complex\"]=carpeta.split('_')[1].upper()\n",
    "    else:\n",
    "        resultado_df[\"Complex\"]=carpeta[0:4]\n",
    "    \n",
    "    # Concatenation of each  total df from each folder\n",
    "    total_df=pd.concat([total_df,resultado_df], ignore_index=True)\n",
    "   \n",
    "\n",
    "total_df.to_csv(directorio_csv + \"pydock4_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ca056",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e088f37",
   "metadata": {},
   "source": [
    "##### 3.2.1  Asignation of the data related to de name of the model: state, model, rank, version and recyle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directorio_csv + \"pydock4_raw.csv\", sep=r'\\t|,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data_frame\n",
    "df = pd.read_csv(directorio_csv + \"pydock4_raw.csv\", sep=r'\\t|,')\n",
    "\n",
    "#information to retrieve with regular expresion\n",
    "state_pattern = re.compile(r'.nrelaxed')\n",
    "version_pattern = re.compile(r\"((deepfold|alphafold2_multimer)_v\\d+)_model\")\n",
    "model_pattern = re.compile(r'model_(\\d+)')\n",
    "rank_pattern = re.compile(r'(rank_(\\d+))|(pred_\\d+)|(ranked_.*)')\n",
    "recycle_pattern = re.compile(r'(_|.)r(\\d{1,})')\n",
    "#seed_pattern = re.compile(r'seed_([0-9]+)\\.')\n",
    "#seed_pattern = re.compile(r'seed_([\\d]+)\\.')\n",
    "seed_pattern = re.compile(r'seed_([0-9]+)(?:\\.|$)')\n",
    "\n",
    "# Defining empty list where the data from the file name will be gather\n",
    "state=[]\n",
    "model=[]\n",
    "version = []\n",
    "recycle = []\n",
    "rank=[]\n",
    "seed=[]\n",
    "# Loop to gather the information entry by entry\n",
    "for linea in (df[\"Name\"].tolist()):\n",
    "    \n",
    "    #State relaxed, unrelaxed\n",
    "    match = state_pattern.search(linea)\n",
    "    if match:\n",
    "        state.append(match.group(0))\n",
    "    else:\n",
    "        state.append(\"relaxed\")\n",
    "    \n",
    "    # Model\n",
    "    match = model_pattern.search(linea)\n",
    "    if match:   \n",
    "        model.append(match.group(1)) \n",
    "    else:\n",
    "        model.append(\"cristal\")   \n",
    "    \n",
    "    #Rank\n",
    "    match = rank_pattern.search(linea)\n",
    "    if match:   \n",
    "        rank.append(match.group(0)) \n",
    "    else:\n",
    "        rank.append(\"unrank\")\n",
    "    \n",
    "    #Version \n",
    "    match = version_pattern.search(linea)\n",
    "    if match: \n",
    "        version.append(match.group(1))\n",
    "    else:\n",
    "        version.append(\"cristal\")\n",
    "    \n",
    "    # Recycle\n",
    "    match = recycle_pattern.search(linea)\n",
    "    if match:\n",
    "        recycle.append(match.group(0)[2:])\n",
    "    else:\n",
    "        recycle.append(\"Seed_0\")          \n",
    "    #Seed\n",
    "    match = seed_pattern.search(linea)\n",
    "    if match:\n",
    "        seed.append(match.group(1))\n",
    "    else:\n",
    "        seed.append(\"-\")\n",
    "\n",
    "# Adding the entries to the dataframe\n",
    "df[\"State\"]=state\n",
    "df[\"Model\"]=model\n",
    "df[\"Rank\"]=rank\n",
    "df[\"Version\"]=version\n",
    "df[\"Recycle\"]=recycle\n",
    "df[\"Seed\"]=seed\n",
    "\n",
    "# Adding additional information \n",
    "\n",
    "df.loc[df[\"Rank\"] == \"unrank\", \"Version\"] = \"alphafold3\" # Alphafold3 models \n",
    "df.loc[(df[\"Model\"] == \"cristal\") & (df[\"Rank\"] == \"unrank\"), [\"Rank\", \"Recycle\", \"State\", \"Version\"]] = \"cristal\" # Defining cristal entries \n",
    "\n",
    "# Old Alphafold2-Multimer- It may be removed\n",
    "lista_valores = [\"pred_0\", \"pred_1\", \"pred_2\", \"pred_3\", \"pred_4\", \"pred_5\"]\n",
    "df.loc[df[\"Rank\"].isin(lista_valores), \"Version\"] = \"Alphafold2\"\n",
    "\n",
    "# To have more available the path for possible future accesion\n",
    "df['Name']= df['Name']+\".pdb\"\n",
    "\n",
    "# Añadimos informacion de los ranked- Unrelevant information, it may be removed\n",
    "df.loc[(df[\"Model\"] == \"cristal\") & (df[\"Rank\"] != \"cristal\"),  \"Version\"] = \"Alphafold2\"\n",
    "df.loc[(df[\"Model\"] == \"cristal\") & (df[\"Rank\"] != \"cristal\"), [\"Model\",  \"Recycle\"]] = \"ranked\"\n",
    "\n",
    "#The models were all relaxed since we used AMBER in openMM\n",
    "df[\"State\"]=\"relaxed\"\n",
    "\n",
    "df_pydock=df\n",
    "df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b692fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock.to_csv(directorio_csv+'/pydock4_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3 Calculation of additional parameters ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56064a",
   "metadata": {},
   "source": [
    "Calculation:\n",
    "- Structured/unstructured: regions and the total number of amino acids in the unstructured regions.\n",
    "- Chain clashes: total is taken, but specific regions can be seen .\n",
    "- Symmetry of identical chains: Rise, degrees in sexagesimal.\n",
    "- Knot detection: disabled by default because it is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e74bc",
   "metadata": {},
   "source": [
    "**Structureed, Clashes and Symmetry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90409965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def add_cryst1_record(pdb_file):\n",
    "    cryst1_line = \"CRYST1   90.000   90.000   90.000  90.00  90.00  90.00 P 1           1\\n\"\n",
    "    with open(pdb_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    if not any(line.startswith('CRYST1') for line in lines):\n",
    "        with open(pdb_file, 'w') as file:\n",
    "            file.write(cryst1_line)\n",
    "            file.writelines(lines)\n",
    "\n",
    "def preprocess_pdb_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pdb'):\n",
    "            pdb_file = os.path.join(directory, filename)\n",
    "            add_cryst1_record(pdb_file)\n",
    "\n",
    "def calculate_chain_clashes(model, chain):\n",
    "    atoms = [atom for atom in model.get_atoms() if atom.get_parent().get_parent() != chain and atom.element != 'H']\n",
    "    ns = NeighborSearch(atoms)\n",
    "    chain_atoms = [atom for atom in chain.get_atoms() if atom.element != 'H']\n",
    "    clashes = 0\n",
    "\n",
    "    for atom in chain_atoms:\n",
    "        neighbors = ns.search(atom.coord, 3.0)\n",
    "        clashes += len([neighbor for neighbor in neighbors if neighbor != atom])\n",
    "    return clashes\n",
    "\n",
    "def count_low_b_factors(chain, threshold):\n",
    "    low_b_factor_count = 0\n",
    "    total_residues = 0\n",
    "    \n",
    "    for residue in chain:\n",
    "        for atom in residue:\n",
    "            if atom.bfactor < threshold:\n",
    "                low_b_factor_count += 1\n",
    "                break\n",
    "        total_residues += 1\n",
    "    \n",
    "    if total_residues == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (low_b_factor_count / total_residues) * 100\n",
    "\n",
    "def calc_rise_and_symmetry(atoms1, atoms2):\n",
    "\n",
    "    # Superimposición de las coordenadas\n",
    "    super_imposer = Superimposer()\n",
    "    super_imposer.set_atoms(atoms1, atoms2)\n",
    "    super_imposer.apply(atoms1)  # Aplicar la rotación a atoms1\n",
    "\n",
    "    # Calcular los centros de masas una vez alineados\n",
    "    aligned_coords1 = np.array([atom.get_coord() for atom in atoms1])\n",
    "    aligned_coords2 = np.array([atom.get_coord() for atom in atoms2])\n",
    "    centroid1 = np.mean(aligned_coords1, axis=0)\n",
    "    centroid2 = np.mean(aligned_coords2, axis=0)\n",
    "    \n",
    "    # Calcular el rise como la distancia entre los centros de masas alineados\n",
    "    rise = np.linalg.norm(centroid1 - centroid2)\n",
    "    \n",
    "    # Obtener la matriz de rotación y calcular el ángulo de simetría\n",
    "    rot_matrix = super_imposer.rotran[0]\n",
    "    rotation = R.from_matrix(rot_matrix)\n",
    "    symmetry_degrees = rotation.magnitude() * (180 / np.pi)  # Convertir de radianes a grados\n",
    "    \n",
    "    return rise, symmetry_degrees\n",
    "\n",
    "def are_sequences_similar(chain1, chain2, threshold=0.8):\n",
    "    seq1 = ''.join([residue.resname for residue in chain1.get_residues()])\n",
    "    seq2 = ''.join([residue.resname for residue in chain2.get_residues()])\n",
    "\n",
    "    aligner = PairwiseAligner()\n",
    "    alignments = aligner.align(seq1, seq2)\n",
    "    best_alignment = alignments[0]\n",
    "    identity = best_alignment.score / max(len(seq1), len(seq2))\n",
    "    \n",
    "    return identity >= threshold\n",
    "\n",
    "def calculate_symmetry(model):\n",
    "    chain_ids = list(model.child_dict.keys())\n",
    "    rises = []\n",
    "    symmetries = []\n",
    "    symmetry_pairs = []\n",
    "    \n",
    "    for i in range(len(chain_ids)):\n",
    "        for j in range(i + 1, len(chain_ids)):\n",
    "            chain1 = model[chain_ids[i]]\n",
    "            chain2 = model[chain_ids[j]]\n",
    "            \n",
    "            if are_sequences_similar(chain1, chain2):\n",
    "                atoms1 = [atom for atom in chain1.get_atoms() if atom.element != 'H']\n",
    "                atoms2 = [atom for atom in chain2.get_atoms() if atom.element != 'H']\n",
    "                \n",
    "                if len(atoms1) > 0 and len(atoms2) > 0:\n",
    "                    rise, symmetry = calc_rise_and_symmetry(atoms1, atoms2)\n",
    "                    rises.append(rise)\n",
    "                    symmetries.append(symmetry)\n",
    "                    symmetry_pairs.append((chain_ids[i], chain_ids[j], rise, symmetry))\n",
    "    \n",
    "    if len(rises) == 0 or len(symmetries) == 0:\n",
    "        return 0, 0, symmetry_pairs\n",
    "    else:\n",
    "        avg_rise = np.mean(rises)\n",
    "        avg_symmetry = np.mean(symmetries)\n",
    "        return avg_rise, avg_symmetry, symmetry_pairs\n",
    "\n",
    "def count_unstructured_amino_acids_and_clashes(pdb_file, bfactor_threshold):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure('X', pdb_file)\n",
    "        model = structure[0]\n",
    "\n",
    "        dssp = DSSP(model, pdb_file)\n",
    "\n",
    "        unstructured_count = 0\n",
    "        max_unstructured_region = 0\n",
    "        total_clashes = 0\n",
    "        clashes_per_chain = {}\n",
    "        low_b_factors_per_chain = {}\n",
    "\n",
    "        for chain in model:\n",
    "            chain_dssp = [dssp[key] for key in dssp.keys() if key[0] == chain.id]\n",
    "\n",
    "            if not chain_dssp:\n",
    "                continue\n",
    "\n",
    "            ss = [aa[2] for aa in chain_dssp]\n",
    "            #ss_string = ''.join(ss)\n",
    "            #print(f\"Chain {chain.id} DSSP data: {ss_string}\") activate only if necessary\n",
    "\n",
    "            first_structured = next((i for i, s in enumerate(ss) if s != '-'), None)\n",
    "            last_structured = next((i, s) for i, s in enumerate(reversed(ss)) if s != '-')\n",
    "            if last_structured is not None:\n",
    "                last_structured = len(ss) - 1 - last_structured[0]\n",
    "\n",
    "            if first_structured is None or last_structured is None:\n",
    "                continue\n",
    "\n",
    "            current_unstructured_count = 0\n",
    "            for s in ss[first_structured:last_structured+1]:\n",
    "                if s == '-':\n",
    "                    current_unstructured_count += 1\n",
    "                else:\n",
    "                    if current_unstructured_count > max_unstructured_region:\n",
    "                        max_unstructured_region = current_unstructured_count\n",
    "                    current_unstructured_count = 0\n",
    "            if current_unstructured_count > max_unstructured_region:\n",
    "                max_unstructured_region = current_unstructured_count\n",
    "\n",
    "            unstructured_count += sum(1 for s in ss[first_structured:last_structured+1] if s == '-')\n",
    "\n",
    "            clashes = calculate_chain_clashes(model, chain)\n",
    "            total_clashes += clashes\n",
    "            clashes_per_chain[chain.id] = clashes\n",
    "\n",
    "            low_b_factors = count_low_b_factors(chain, bfactor_threshold)\n",
    "            low_b_factors_per_chain[chain.id] = low_b_factors\n",
    "\n",
    "        avg_rise, avg_symmetry, symmetry_pairs = calculate_symmetry(model)\n",
    "\n",
    "        return unstructured_count, max_unstructured_region, total_clashes, clashes_per_chain, low_b_factors_per_chain, avg_rise, avg_symmetry, symmetry_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdb_file}: {e}\")\n",
    "        return None, None, None, None, None, None, None, None\n",
    "\n",
    "def process_pdb_file(args):\n",
    "    pdb_file, bfactor_threshold = args\n",
    "    results = count_unstructured_amino_acids_and_clashes(pdb_file, bfactor_threshold)\n",
    "    if results[0] is not None:\n",
    "        unstructured_count, max_unstructured_region, total_clashes, clashes_per_chain, low_b_factors_per_chain, avg_rise, avg_symmetry, symmetry_pairs = results\n",
    "        row = {\n",
    "            'Name': os.path.basename(pdb_file),\n",
    "            'Unstructured_count': unstructured_count,\n",
    "            'Max_unstructured_region': max_unstructured_region,\n",
    "            'Total_clashes': total_clashes,\n",
    "            'Average_rise': avg_rise,\n",
    "            'Average_symmetry': avg_symmetry\n",
    "        }\n",
    "        for chain_id, clashes in clashes_per_chain.items():\n",
    "            row[f'Clashes_chain_{chain_id}'] = clashes\n",
    "        for chain_id, low_b_factors in low_b_factors_per_chain.items():\n",
    "            row[f'Low_B_factors_chain_{chain_id}'] = low_b_factors\n",
    "        \n",
    "        for chain1, chain2, rise, symmetry in symmetry_pairs:\n",
    "            row[f'Rise_{chain1}_{chain2}'] = rise\n",
    "            row[f'Symmetry_{chain1}_{chain2}'] = symmetry\n",
    "        \n",
    "        return row\n",
    "    return None\n",
    "\n",
    "def main(path, directories, bfactor_threshold, num_threads):\n",
    "    #patron  = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.pdb'\n",
    "    patron = r'(.*(\\d+)\\.pdb$)'\n",
    "    for directory in directories:\n",
    "        directory = os.path.join(path, directory)\n",
    "        preprocess_pdb_files(directory)\n",
    "\n",
    "    pdb_files = []\n",
    "    for directory in directories:\n",
    "        directory = os.path.join(path, directory)\n",
    "        for filename in os.listdir(directory):\n",
    "            if re.match(patron, filename):\n",
    "                pdb_file = os.path.join(directory, filename)\n",
    "                pdb_files.append((pdb_file, bfactor_threshold))\n",
    "\n",
    "    with Pool(num_threads) as pool:\n",
    "        data = pool.map(process_pdb_file, pdb_files)\n",
    "\n",
    "    data = [row for row in data if row is not None]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Usage of all the functions\n",
    "\n",
    "bfactor_threshold = 50  # Umbral de B-factor\n",
    "num_threads = 20\n",
    "df_loop_clashes = main(directorio,carpetas,bfactor_threshold, num_threads)\n",
    "\n",
    "# Imprime el dataframe resultante\n",
    "print(df_loop_clashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12932c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join df_loop_clashes and df_pydock by Name\n",
    "df_pydock = df_pydock.merge(df_loop_clashes, on= 'Name', how='left')\n",
    "df_pydock\n",
    "#T254 No Knots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de5feb",
   "metadata": {},
   "source": [
    "**Knot detection**\n",
    "\n",
    "Could lead to errors, comment or uncomment  the following function if there is any problem or lasts more than 40 min\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Comment begins\n",
    "\n",
    "#def check_knots_and_get_info(pdb_file):\n",
    " #    command = [\"knot_pull_check\", \"-kq\", pdb_file]                                              \n",
    "  #   try:\n",
    "   #      result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    #     output = result.stdout.strip()\n",
    "     #   \n",
    "      #   # Analiza la salida para determinar si contiene '#'\n",
    "       #  if '#' in output:\n",
    "        #     return (pdb_file, 'yes')\n",
    "         #else:\n",
    "          #   return (pdb_file, 'no')\n",
    "    # except subprocess.CalledProcessError as e:\n",
    "        # print(f\"Error executing command: {e}\")                                                 \n",
    "    #return (pdb_file, 'no')  # Asumimos 'no' si hay un error al ejecutar el comando\n",
    "\n",
    "#### Comments ends, execute below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b657eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## descomentar si es necesario##\n",
    "def check_knots_and_get_info(pdb_file):\n",
    "                                                    \n",
    "     return (pdb_file, 'no')  # Asumimos 'no' si hay un error al ejecutar el comando\n",
    "\n",
    "def process_pdb_files(path, directories, num_workers):\n",
    "    patron = r'(.*(\\d+)\\.pdb$)'\n",
    "   # patron = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.pdb'\n",
    "    data = []\n",
    "\n",
    "    pdb_files = []\n",
    "    for directory in directories:\n",
    "        directory_path = os.path.join(path, directory)\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if re.match(patron, filename):\n",
    "                pdb_file = os.path.join(directory_path, filename)\n",
    "                pdb_files.append(pdb_file)\n",
    "    \n",
    "    # Usar ProcessPoolExecutor para paralelizar la ejecución con un número específico de trabajadores\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_pdb = {executor.submit(check_knots_and_get_info, pdb_file): pdb_file for pdb_file in pdb_files}\n",
    "        for future in as_completed(future_to_pdb):\n",
    "            pdb_file = future_to_pdb[future]\n",
    "            try:\n",
    "                filename, knot_info = future.result()\n",
    "                row = {\n",
    "                    'Name': os.path.basename(filename),\n",
    "                    'Knots': knot_info\n",
    "                }\n",
    "                data.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {pdb_file}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "num_workers = 20  # Número de hilos a utilizar\n",
    "df_knots = process_pdb_files(directorio,carpetas,num_workers)\n",
    "\n",
    "df_knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c259431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join df_knots and df_pydock by Name\n",
    "df_pydock = df_pydock.merge(df_knots, on= 'Name', how='left')\n",
    "df_pydock.to_csv(directorio_csv+'/pydock4_additional.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a72566e0",
   "metadata": {},
   "source": [
    "#### 3.3 Log.txt information retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcec9c",
   "metadata": {},
   "source": [
    "The information related to the pLDDT, pTM, ipTM and tol will be gathered in a single dataframe that will be merged with df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock\n",
    "# Folders of all models\n",
    "carpetas_log = [nombre for nombre in os.listdir(directorio) if os.path.isdir(os.path.join(directorio, nombre))]\n",
    "#carpetas_log.remove('Version1')\n",
    "carpetas_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe columns\n",
    "columns = [\"Complex\",\"Model\",\"State\",'Version', 'Recycle', 'pLDDT', 'pTM', 'ipTM', 'tol','Seed']\n",
    "\n",
    "# Patrons in the text to gather the information\n",
    "#complex_pattern = re.compile(r'((T|t)\\/.*_A)') \n",
    "rank_pattern = re.compile(r'(rank_(\\d+))|(pred_\\d+)|(ranked_.*)')\n",
    "model_pattern = re.compile(r'model_(\\d+)')\n",
    "state_pattern = re.compile(r'rank')\n",
    "version_pattern = re.compile(r\"((deepfold|alphafold2_multimer)_v\\d+)_model\")\n",
    "recycle_pattern = re.compile(r'recycle=(\\d+)')\n",
    "plddt_pattern = re.compile(r'pLDDT=([\\d.]+)')\n",
    "ptm_pattern = re.compile(r'pTM=([\\d.]+)')\n",
    "iptm_pattern = re.compile(r'ipTM=([\\d.]+)')\n",
    "tol_pattern = re.compile(r'tol=([\\d.]+)')\n",
    "seed_pattern = re.compile(r'seed_([\\d.]+)')\n",
    "name_pattern = re.compile(r\"(fold_t\\d+_\\d+_model_\\d+)\")\n",
    "df_log=pd.DataFrame()\n",
    "\n",
    "for carpeta in carpetas_log:\n",
    "    directorio_log=f\"{directorio}/{carpeta}/log.txt\"\n",
    "    # Loading the archive\n",
    "    with open(directorio_log, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Value extraction\n",
    "    #name = None\n",
    "    complex=None\n",
    "    model=None\n",
    "    version = None\n",
    "    state=None\n",
    "    recycle = None\n",
    "    plddt = None\n",
    "    ptm = None\n",
    "    iptm = None\n",
    "    tol = None\n",
    "    seed = None\n",
    "    data=[]\n",
    "    for line in lines:\n",
    "        \n",
    "        # # Complex\n",
    "        # match = complex_pattern.search(line)\n",
    "        # if match:\n",
    "        #     print()\n",
    "        #     complex = match.group(0)\n",
    "        #     complex=complex[2:-2]\n",
    "\n",
    "        #Name\n",
    "        # match = name_pattern.search(line)\n",
    "        # if match:\n",
    "        #     name = match.group(1)+'.pdb'\n",
    "        # else:\n",
    "        #     name =directorio_log \n",
    "        #State\n",
    "        match = state_pattern.search(line)\n",
    "        if match:\n",
    "            state=\"relaxed\"\n",
    "        else:\n",
    "            state=\"unrelaxed\"\n",
    "\n",
    "        # Model\n",
    "        match = model_pattern.search(line)\n",
    "        if match:\n",
    "            model= match.group(1)\n",
    "        \n",
    "        # Version\n",
    "        match = version_pattern.search(line)\n",
    "        if match:\n",
    "            version = match.group(1)\n",
    "        else:\n",
    "            version = 'alphafold3'\n",
    "            \n",
    "        # Recycle\n",
    "        match = recycle_pattern.search(line)\n",
    "        if match:\n",
    "            recycle = match.group(1)\n",
    "        else:\n",
    "            recycle = 'Seed_0'\n",
    "        \n",
    "        #  pLDDT\n",
    "        match = plddt_pattern.search(line)\n",
    "        if match:\n",
    "            plddt = match.group(1)\n",
    "        else:\n",
    "            plddt = None\n",
    "        \n",
    "        #  pTM\n",
    "        match = ptm_pattern.search(line)\n",
    "        if match:\n",
    "            ptm = match.group(1)\n",
    "        else:\n",
    "            ptm=None\n",
    "        \n",
    "        #  ipTM\n",
    "        match = iptm_pattern.search(line)\n",
    "        if match:\n",
    "            iptm = match.group(1)\n",
    "        else:\n",
    "            iptm=None\n",
    "        \n",
    "        #  tol\n",
    "        match = tol_pattern.search(line)\n",
    "        if match:\n",
    "            tol = match.group(1)\n",
    "        else:\n",
    "            tol=\"-\"\n",
    "        \n",
    "        #seed\n",
    "        match = seed_pattern.search(line)\n",
    "        if match:\n",
    "            seed = match.group(1)\n",
    "        else:\n",
    "            seed=\"-\"\n",
    "        \n",
    "        # rank\n",
    "        match = rank_pattern.search(line)\n",
    "        if match:   \n",
    "            recycle = 'Seed_0'\n",
    "        # Guardar los valores en el DataFrame\n",
    "        data.append([complex,model,state,version, recycle, plddt, ptm, iptm, tol,seed])\n",
    "\n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Model Conficende calculated as AF2-Multimer paper\n",
    "    df['ipTM'] = pd.to_numeric(df['ipTM'], errors='coerce')\n",
    "    df['pTM'] = pd.to_numeric(df['pTM'], errors='coerce')\n",
    "    df['Model_confidence'] = 0.8 * df['ipTM'] + 0.2 * df['pTM']\n",
    "    if carpeta.startswith('fold'):\n",
    "        df[\"Complex\"]=carpeta.split('_')[1].upper()\n",
    "    else:\n",
    "        df[\"Complex\"]=carpeta[0:4]\n",
    "    df = df.dropna(subset=['Model_confidence'])\n",
    "    df['pLDDT'] = pd.to_numeric(df['pLDDT'], errors='coerce')\n",
    "\n",
    "    # Ensambling the df_log to gather all the information\n",
    "    df_log=pd.concat([df_log,df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aef76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The models were all relaxed\n",
    "df_log[\"State\"]=\"relaxed\"\n",
    "df_log.to_csv(directorio_csv+'/log_all.csv', index=False)\n",
    "df_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24f4d664",
   "metadata": {},
   "source": [
    "### 4.  Final fusion and Adjusments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44be3403",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">Now we ensemble a new_dataframe to collect all the data obtained during the calculation of for a posterior statistical analysis\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ffa3f3a",
   "metadata": {},
   "source": [
    "#### 4.1 Checking for possible issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d11e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataframes\n",
    "df_pydock =pd.read_csv(directorio_csv+'/pydock4_additional.csv')\n",
    "df_log=pd.read_csv(directorio_csv+'/log_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22307bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e77c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking for the length of dataframes, the should match, if not revised\n",
    "unicos=set(df_pydock[\"Complex\"])\n",
    "for complejo in unicos:\n",
    "    print(complejo)\n",
    "    n_pydock=len(df_pydock[df_pydock[\"Complex\"]==complejo])\n",
    "    n_log=len(df_log[df_log[\"Complex\"]==complejo])\n",
    "    print(\"Pydock:\",n_pydock, \" Log:\",n_log,\"Difference:\",n_pydock-n_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb6c7f24",
   "metadata": {},
   "source": [
    "#### 4.2 Merging dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29c9bf0e",
   "metadata": {},
   "source": [
    "Determaining which columns are diferent and merging by the common ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the  shared columns\n",
    "columna4=(df_pydock.columns).tolist()\n",
    "columna3=(df_log.columns).tolist()\n",
    "compartidos2=list(set(columna4).intersection(columna3))\n",
    "\n",
    "# Coercing to have the same type\n",
    "df_pydock[compartidos2]=df_pydock[compartidos2].astype(str)\n",
    "df_log[compartidos2]=df_log[compartidos2].astype(str)\n",
    "\n",
    "# Merging the values\n",
    "merged_df2 = df_pydock.merge(df_log, on= compartidos2, how='left')\n",
    "print (compartidos2)\n",
    "\n",
    "# Savind the results\n",
    "merged_df2.to_csv(directorio_csv+'/merged_df2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bee56",
   "metadata": {},
   "source": [
    "#### 4.3 Filter the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a895dfd",
   "metadata": {},
   "source": [
    "In this section we apply the filter_pydock_advanced function to gather the best models according to:\n",
    "\n",
    "- Max_unstructured_region: \n",
    "\n",
    "- Total_clashes: how many clashes there are\n",
    "\n",
    "- res_conditions: Indicates the % of residues which have a ppLDT<50 in a certain chain\n",
    "\n",
    "- Knots_value: \"yes\" or 'no' to have in consideration the existence of knots\n",
    "\n",
    "-  symmetry_conditions: Symetry interval to consider repited structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pydock_advanced(\n",
    "    df, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "    symmetry_conditions, res_conditions, invert=False\n",
    "):\n",
    "    # Filtro inicial basado en Knots_value, Max_unstructured_region y Total_clashes\n",
    "    main_filter = (\n",
    "        (df['Knots'] == Knots_value) &\n",
    "        (df['Max_unstructured_region'] <= Max_unstructured_region) &\n",
    "        (df['Total_clashes'] <= Total_clashes)\n",
    "    )\n",
    "\n",
    "    # Construye las condiciones de simetría dinámicamente\n",
    "    symmetry_filter = None\n",
    "    for condition in symmetry_conditions:\n",
    "        Symmetry_col, up, low = condition\n",
    "        if Symmetry_col not in df.columns:\n",
    "            warnings.warn(f\"Column '{Symmetry_col}' does not exist in the DataFrame.\")\n",
    "            continue\n",
    "        current_filter = (\n",
    "            (df[Symmetry_col].between(-up, -low)) | \n",
    "            (df[Symmetry_col].between(low, up))\n",
    "        )\n",
    "        if symmetry_filter is None:\n",
    "            symmetry_filter = current_filter\n",
    "        else:\n",
    "            symmetry_filter |= current_filter\n",
    "    \n",
    "    if symmetry_filter is not None:\n",
    "        main_filter &= symmetry_filter\n",
    "\n",
    "    # Construye las condiciones de Res_with_low_pLDDT dinámicamente\n",
    "    res_filter = None\n",
    "    for condition in res_conditions:\n",
    "        Res_col, threshold = condition\n",
    "        if Res_col not in df.columns:\n",
    "            warnings.warn(f\"Column '{Res_col}' does not exist in the DataFrame.\")\n",
    "            continue\n",
    "        # Verificar si el valor en df[Res_col] es 0, en cuyo caso se ignora el filtro\n",
    "        if df[Res_col].eq(0).all():\n",
    "            continue\n",
    "        current_filter = (df[Res_col] <= threshold)\n",
    "        if res_filter is None:\n",
    "            res_filter = current_filter\n",
    "        else:\n",
    "            res_filter &= current_filter\n",
    "    \n",
    "    if res_filter is not None:\n",
    "        main_filter &= res_filter\n",
    "\n",
    "    # Aplicar el filtro inverso si invert es True\n",
    "    if invert:\n",
    "        filtered_df = df[~main_filter]\n",
    "    else:\n",
    "        filtered_df = df[main_filter]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085b056",
   "metadata": {},
   "source": [
    "Aplication of the function. First load the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df2=pd.read_csv(directorio_csv+'/merged_df2.csv')\n",
    "merged_df2.drop_duplicates(inplace=True,subset=\"Name\")\n",
    "merged_df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19a916",
   "metadata": {},
   "source": [
    " Second look at the dataframe with an extension like datawrangler to see wich values of the columns is the best to use to filter the desired number of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a321b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T309 example\n",
    "Max_unstructured_region, Total_clashes=5, 30,\n",
    "bfactor_1=1\n",
    "res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 185, 175)] \n",
    "\n",
    "\n",
    "filtered_df = filter_pydock_advanced(\n",
    "    merged_df2, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "    symmetry_conditions=symmetry_conditions,\n",
    "    res_conditions=res_conditions\n",
    ")\n",
    "\n",
    "result_df_inverted = filter_pydock_advanced(\n",
    "    merged_df2, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "    symmetry_conditions=symmetry_conditions,\n",
    "    res_conditions=res_conditions,\n",
    "    invert=True\n",
    ")\n",
    "print(len(filtered_df))\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(directorio_csv+'/pydock4_all_filtered.csv', index=False)\n",
    "result_df_inverted.to_csv(directorio_csv+'/pydock4_all_filtered_inv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01974281",
   "metadata": {},
   "source": [
    "History of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc539311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 4, 55, \\\n",
    "#     [('Low_B_factors_chain_A', 20), ('Low_B_factors_chain_B', 20)], \\\n",
    "#     'no', [('Symmetry_A_B', 185, 175)] # T242 incluimos la simtria  180-+5  E_F\n",
    "#Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 8, 55, \\\n",
    "#     [('Low_B_factors_chain_A', 7), ('Low_B_factors_chain_B', 7)], \\\n",
    "#     'no', [('Symmetry_A_B', 185, 175)]# T244 incluimos, no se calcula Symmetry \n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 55, \\\n",
    "    # [('Low_B_factors_chain_A', 9), ('Low_B_factors_chain_B', 12)], \\\n",
    "    # 'no', [('Symmetry_A_B', 185, 175)]# T248 incluimos, no se calcula Symmetry \n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 500, \\\n",
    "#     [('Low_B_factors_chain_A', 70), ('Low_B_factors_chain_B', 70), ('Low_B_factors_chain_C', 70)], \\\n",
    "#     'no', [('Symmetry_B_C', 125, 115),('Symmetry_A_B', 125, 115)]# T250 T252 incluimos, se calcula Symmetry \n",
    "#Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 300, [('Low_B_factors_chain_A', 10), ('Low_B_factors_chain_B', 10), ('Low_B_factors_chain_C', 10),('Low_B_factors_chain_D', 10),('Low_B_factors_chain_E', 10)], 'no', [('Symmetry_B_C', 125, 115),('Symmetry_A_B', 125, 115)]# T256 \n",
    "\n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 500, \\\n",
    "#     [('Low_B_factors_chain_A', 10), ('Low_B_factors_chain_B', 10), ('Low_B_factors_chain_C', 10),('Low_B_factors_chain_D', 10)], \\\n",
    "#     'no', [('Symmetry_A_B', 185, 175)]# T254 T255 incluimos, se calcula Symmetry \n",
    "\n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 20, 300, \\\n",
    "#     [('Low_B_factors_chain_A', 66), ('Low_B_factors_chain_B', 66), ('Low_B_factors_chain_C', 66),('Low_B_factors_chain_D', 66),('Low_B_factors_chain_E', 66)], \\\n",
    "#     'no', [('Symmetry_A_F', 185, 175)]# T262, se uss Symmetry_A_F que no exite para no calcular filtro por Symmetria \n",
    "\n",
    "### T264, se usa Symmetry_A_Z que no exite para no calcular filtro por Symmetria ##\n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 24, 1000, \\\n",
    "#     [('Low_B_factors_chain_A', 24),('Low_B_factors_chain_B', 24),('Low_B_factors_chain_C', 24),('Low_B_factors_chain_D', 20)], \\\n",
    "#     'no', [('Symmetry_A_Z', 185, 175)]\n",
    "\n",
    "\n",
    "### T266, se usa Symmetry_A_F que no exite para no calcular filtro por Symmetria ##\n",
    "# Max_unstructured_region, Total_clashes,\\\n",
    "# res_conditions,\\\n",
    "# Knots_value, symmetry_conditions =\\\n",
    "#     20, 60, \\\n",
    "#     [('Low_B_factors_chain_A', 20)], \\\n",
    "#     'no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "\n",
    "\n",
    "## Customizable\n",
    "# Max_unstructured_region, Total_clashes=20, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 20)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "# Max_unstructured_region, Total_clashes=20, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 20)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "#T168\n",
    "# Max_unstructured_region, Total_clashes=7, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 80)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "# filtered_df = filter_pydock_advanced(\n",
    "#     merged_df2, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "#     symmetry_conditions=symmetry_conditions,\n",
    "#     res_conditions=res_conditions\n",
    "#)\n",
    "#T170\n",
    "# Max_unstructured_region, Total_clashes=7, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 80)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "# #T172\n",
    "# merged_df2=pd.read_csv(directorio_csv+'/pydock4_all.csv')\n",
    "# Max_unstructured_region, Total_clashes=12, 5000\n",
    "# res_conditions=[('Clashes_chain_A', 600)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_B_D', 185, 175),()]\n",
    "\n",
    "# #T280\n",
    "# merged_df2=pd.read_csv(directorio_csv+'/merged_df2.csv')\n",
    "# Max_unstructured_region, Total_clashes=8, 150\n",
    "# res_conditions=[('Low_B_factors_chain_A', 8),('Low_B_factors_chain_B', 8),('Low_B_factors_chain_C', 8),('Low_B_factors_chain_D', 8)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 185, 175)]\n",
    "\n",
    "#T288\n",
    "# Max_unstructured_region, Total_clashes=6, 110,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 21),('Low_B_factors_chain_B', 21),(\"Low_B_factors_chain_C\",21)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 140, 100),(\"Symmetry_B_C\",140, 100)]\n",
    "\n",
    "#T290\n",
    "# Max_unstructured_region, Total_clashes=2, 97,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 5),('Low_B_factors_chain_B', 5),(\"Low_B_factors_chain_C\",5),(\"Low_B_factors_chain_D\",5),(\"Low_B_factors_chain_E\",5),(\"Low_B_factors_chain_F\",5)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 65, 55),(\"Symmetry_B_C\",65, 55)]\n",
    "\n",
    "# #T290\n",
    "# Max_unstructured_region, Total_clashes=4, 50,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 5),('Low_B_factors_chain_B', 50)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 65, 55)]\n",
    "\n",
    "# #T292\n",
    "# Max_unstructured_region, Total_clashes=7, 400,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 14),('Low_B_factors_chain_B', 14),(\"Low_B_factors_chain_C\",14),(\"Low_B_factors_chain_D\",14),(\"Low_B_factors_chain_E\",14),(\"Low_B_factors_chain_F\",14),(\"Low_B_factors_chain_G\",14),(\"Low_B_factors_chain_H\",14),(\"Low_B_factors_chain_I\",14)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 135, 105),(\"Symmetry_B_C\",135, 105)]\n",
    "\n",
    "# # T282\n",
    "# Max_unstructured_region, Total_clashes=6, 190,\n",
    "# bfactor_1=3\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1),(\"Low_B_factors_chain_D\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_C', 190, 170),(\"Symmetry_B_D\",190, 170)]\n",
    "\n",
    "# T286\n",
    "# Max_unstructured_region, Total_clashes=8, 400,\n",
    "# bfactor_1=7\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1),(\"Low_B_factors_chain_D\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_B_C\",140, 100)]\n",
    "\n",
    "# T296\n",
    "# Max_unstructured_region, Total_clashes=5, 32,\n",
    "# #bfactor_1=7\n",
    "# res_conditions=[('Low_B_factors_chain_A', 5),('Low_B_factors_chain_B', 100)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_B_C\",140, 100)]\n",
    "\n",
    "# T294\n",
    "# Max_unstructured_region, Total_clashes=5, 130,\n",
    "# #bfactor_1=7\n",
    "# res_conditions=[('Low_B_factors_chain_A', 2),('Low_B_factors_chain_B', 2),(\"Low_B_factors_chain_C\",7),(\"Low_B_factors_chain_D\",7), (\"Low_B_factors_chain_E\",17),(\"Low_B_factors_chain_F\",17)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",190, 160), (\"Symmetry_C_D\",190, 160), (\"Symmetry_E_F\",50, 0)]\n",
    "\n",
    "# T298 \n",
    "# Max_unstructured_region, Total_clashes=6, 90,\n",
    "# bfactor_1=2.5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",125, 115), (\"Symmetry_B_C\",125, 115)]\n",
    "\n",
    "# T300\n",
    "# Max_unstructured_region, Total_clashes=6, 120,\n",
    "# bfactor_1=5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",185, 175), (\"Symmetry_C_D\",185, 175)]\n",
    "\n",
    "# T304 \n",
    "# Max_unstructured_region, Total_clashes=6, 16,\n",
    "# bfactor_1=5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_H_B\",185, 175)] \n",
    "\n",
    "# T302 \n",
    "# Max_unstructured_region, Total_clashes=4, 280,\n",
    "# bfactor_1=5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",125, 115)] \n",
    "\n",
    "# T306\n",
    "# Max_unstructured_region, Total_clashes=3, 10,\n",
    "# bfactor_1=1.6\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",125, 115)] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Max_unstructured_region,Total_clashes,Res_with_low_pLDDT_A,Res_with_low_pLDDT_B,Knots_value = [10,90,20,20,'no']# T236\n",
    "# #Max_unstructured_region,Total_clashes,Res_with_low_pLDDT_A,Res_with_low_pLDDT_B,Knots_value = [5,70,15,20,'no']# T238\n",
    "# Max_unstructured_region,Total_clashes,Res_with_low_pLDDT_E,Res_with_low_pLDDT_F,Knots_value = [5,800,20,20,'no']# T240\n",
    "\n",
    "# filtered_df = merged_df2[\n",
    "#     (merged_df2['Knots'] == 'no') &\n",
    "#     (merged_df2['Max_unstructured_region'] <= Max_unstructured_region) &\n",
    "#     (merged_df2['Total_clashes'] <= Total_clashes) &\n",
    "#     #(merged_df2['Res_with_low_pLDDT_A'] <= Res_with_low_pLDDT_A) & #T236 T238\n",
    "#     #(merged_df2['Res_with_low_pLDDT_B'] <= Res_with_low_pLDDT_B)   #T236 T238\n",
    "#     (merged_df2['Res_with_low_pLDDT_E'] <= Res_with_low_pLDDT_E) &  #T240\n",
    "#     (merged_df2['Res_with_low_pLDDT_F'] <= Res_with_low_pLDDT_F)    #T240\n",
    "# ]\n",
    "\n",
    "# result_df_inverted = merged_df2[\n",
    "#     ~(\n",
    "#         (merged_df2['Knots'] == Knots_value) &\n",
    "#         (merged_df2['Max_unstructured_region'] <= Max_unstructured_region) &\n",
    "#         (merged_df2['Total_clashes'] <= Total_clashes) &\n",
    "#         #(merged_df2['Res_with_low_pLDDT_A'] <= Res_with_low_pLDDT_A) & #T236 T238\n",
    "#         #(merged_df2['Res_with_low_pLDDT_B'] <= Res_with_low_pLDDT_B)   #T236 T238\n",
    "#         (merged_df2['Res_with_low_pLDDT_E'] <= Res_with_low_pLDDT_E) &  #T240\n",
    "#         (merged_df2['Res_with_low_pLDDT_F'] <= Res_with_low_pLDDT_F)    #T240\n",
    "#     )\n",
    "# ]\n",
    "# # Resultado final\n",
    "# filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df_inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c76b3",
   "metadata": {},
   "source": [
    "### 5. Ranking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ca1f0",
   "metadata": {},
   "source": [
    "#### 5.1 Z-score of pydock and Model confidence of the selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27adfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm=pd.read_csv(directorio_csv+'/pydock4_all_filtered.csv')\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecesary columns\n",
    "columnas=['Conf','RANK']\n",
    "df_norm.drop(columnas, axis=1, inplace=True)\n",
    "df_norm.dropna(subset=[\"Complex\"],inplace=True)\n",
    "\n",
    "# Removing duplicates\n",
    "df_norm=df_norm.drop_duplicates(subset=[\"Name\"],keep=\"first\")\n",
    "duplicados = df_norm[df_norm.duplicated(subset=[\"Name\",\"Version\",\"Complex\",\"Recycle\",\"State\"])]\n",
    "\n",
    "# Adding Total2 column\n",
    "df_norm[\"Total2\"]=df_norm[\"VDW\"]+df_norm[\"Ele\"]+df_norm[\"Desolv\"] \n",
    "\n",
    "# Z-Score individuales, inicializacion\n",
    "df_norm[\"MCZ-Score\"] = 0 # Z-score de model_conficence\n",
    "df_norm[\"PLDDTZ-Score\"] = 0 # Z-score de pLDDT\n",
    "df_norm[\"TEZ-Score\"] = 0 # Z-score de Total\n",
    "df_norm[\"TE2Z-Score\"] = 0 # Z-score de Total2\n",
    "\n",
    "# Suma de Z-Score, inicialicion\n",
    "df_norm[\"Sum_Z\"] = 0 # Z-score Model confidence + Total\n",
    "df_norm[\"Sum2_Z\"] = 0 # Z-score Model confidence + Total2\n",
    "df_norm[\"Z-PLT\"] = 0 # Z-score de pLDDT + Total\n",
    "df_norm[\"Z-PLT2\"]= 0 # Z-score de pLDDT + Total2\n",
    "\n",
    "# Ranking Z-Score, inicializacion\n",
    "df_norm[\"Ranking_Z\"] = 0 # Ranking de Sum_Z\n",
    "df_norm[\"Ranking2_Z\"] = 0 # Ranking de Sum2_Z\n",
    "df_norm[\"Ranking_PLT\"] = 0 # Ranking de Z-PLT\n",
    "df_norm[\"Ranking_PLT2\"] = 0 # Ranking de Z-PLT2\n",
    "\n",
    "# Calculo de medias y desviaciones segun complejo\n",
    "grouped = df_norm.groupby([\"Complex\"])\n",
    "medias=grouped.mean()\n",
    "sdesv=grouped.std()\n",
    "\n",
    "# Z-Score individuales\n",
    "for name, group in grouped:\n",
    "    # Calculamos Z_score de model_conficence y total energy\n",
    "    df_norm.loc[group.index,[\"MCZ-Score\"]] = (group[\"Model_confidence\"]-medias.loc[name,\"Model_confidence\"])/sdesv.loc[name,\"Model_confidence\"]\n",
    "    df_norm.loc[group.index,[\"TEZ-Score\"]] = (group[\"Total\"]-medias.loc[name,\"Total\"])/sdesv.loc[name,\"Total\"]\n",
    "    df_norm.loc[group.index,[\"TE2Z-Score\"]] = (group[\"Total2\"]-medias.loc[name,\"Total2\"])/sdesv.loc[name,\"Total2\"]\n",
    "    df_norm.loc[group.index,[\"PLDDTZ-Score\"]] = (group[\"pLDDT\"]-medias.loc[name,\"pLDDT\"])/sdesv.loc[name,\"pLDDT\"]\n",
    "\n",
    "# Suma de Z-Score\n",
    "df_norm.loc[:,\"Sum_Z\"]=df_norm.loc[:,\"MCZ-Score\"]-df_norm.loc[:,\"TEZ-Score\"]\n",
    "df_norm.loc[:,\"Sum2_Z\"]=df_norm.loc[:,\"MCZ-Score\"]-df_norm.loc[:,\"TE2Z-Score\"]\n",
    "df_norm.loc[:,\"Z-PLT\"]=df_norm.loc[:,\"PLDDTZ-Score\"]-df_norm.loc[:,\"TEZ-Score\"]\n",
    "df_norm.loc[:,\"Z-PLT2\"]=df_norm.loc[:,\"PLDDTZ-Score\"]-df_norm.loc[:,\"TE2Z-Score\"]\n",
    "\n",
    "# Ranking Z-Score\n",
    "for name, group in grouped:\n",
    "    df_norm.loc[group.index,\"Ranking_Z\"]=df_norm.loc[group.index,\"Sum_Z\"].rank(ascending=False)\n",
    "    df_norm.loc[group.index,\"Ranking2_Z\"]=df_norm.loc[group.index,\"Sum2_Z\"].rank(ascending=False)\n",
    "    df_norm.loc[group.index,\"Ranking_PLT\"]=df_norm.loc[group.index,\"Z-PLT\"].rank(ascending=False)\n",
    "    df_norm.loc[group.index,\"Ranking_PLT2\"]=df_norm.loc[group.index,\"Z-PLT2\"].rank(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.to_csv(directorio_csv + \"/df_norm_\"+Target_name+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43829eb3",
   "metadata": {},
   "source": [
    "#### 5.2 Z-score of pydock and Model confidence of the inversed dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e71c39",
   "metadata": {},
   "source": [
    "In case there are too few in the filtered results, add fillers; they are not important, just for the submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_inv=pd.read_csv(directorio_csv+'/pydock4_all_filtered_inv.csv')\n",
    "\n",
    "# Removing unnecesary columns\n",
    "columnas=['Conf','RANK']\n",
    "#df_norm_inv.drop(columnas, axis=1, inplace=True)\n",
    "#df_norm_inv.dropna(subset=[\"Complex\"],inplace=True)\n",
    "\n",
    "# Removing duplicates\n",
    "df_norm_inv=df_norm_inv.drop_duplicates(subset=[\"Name\"],keep=\"first\")\n",
    "duplicados = df_norm_inv[df_norm_inv.duplicated(subset=[\"Name\",\"Version\",\"Complex\",\"Recycle\",\"State\"])]\n",
    "\n",
    "# Adding Total2 column\n",
    "df_norm_inv[\"Total2\"]=df_norm_inv[\"VDW\"]+df_norm_inv[\"Ele\"]+df_norm_inv[\"Desolv\"] \n",
    "\n",
    "# Z-Score individuales, inicializacion\n",
    "df_norm_inv[\"MCZ-Score\"] = 0 # Z-score de model_conficence\n",
    "df_norm_inv[\"PLDDTZ-Score\"] = 0 # Z-score de pLDDT\n",
    "df_norm_inv[\"TEZ-Score\"] = 0 # Z-score de Total\n",
    "df_norm_inv[\"TE2Z-Score\"] = 0 # Z-score de Total2\n",
    "\n",
    "# Suma de Z-Score, inicialicion\n",
    "df_norm_inv[\"Sum_Z\"] = 0 # Z-score Model confidence + Total\n",
    "df_norm_inv[\"Sum2_Z\"] = 0 # Z-score Model confidence + Total2\n",
    "df_norm_inv[\"Z-PLT\"] = 0 # Z-score de pLDDT + Total\n",
    "df_norm_inv[\"Z-PLT2\"]= 0 # Z-score de pLDDT + Total2\n",
    "\n",
    "# Ranking Z-Score, inicializacion\n",
    "df_norm_inv[\"Ranking_Z\"] = 0 # Ranking de Sum_Z\n",
    "df_norm_inv[\"Ranking2_Z\"] = 0 # Ranking de Sum2_Z\n",
    "df_norm_inv[\"Ranking_PLT\"] = 0 # Ranking de Z-PLT\n",
    "df_norm_inv[\"Ranking_PLT2\"] = 0 # Ranking de Z-PLT2\n",
    "\n",
    "# Calculo de medias y desviaciones segun complejo\n",
    "grouped = df_norm_inv.groupby([ \"Complex\"])\n",
    "medias=grouped.mean()\n",
    "sdesv=grouped.std()\n",
    "\n",
    "# Z-Score individuales\n",
    "for name, group in grouped:\n",
    "    # Calculamos Z_score de model_conficence y total energy\n",
    "    df_norm_inv.loc[group.index,[\"MCZ-Score\"]] = (group[\"Model_confidence\"]-medias.loc[name,\"Model_confidence\"])/sdesv.loc[name,\"Model_confidence\"]\n",
    "    df_norm_inv.loc[group.index,[\"TEZ-Score\"]] = (group[\"Total\"]-medias.loc[name,\"Total\"])/sdesv.loc[name,\"Total\"]\n",
    "    df_norm_inv.loc[group.index,[\"TE2Z-Score\"]] = (group[\"Total2\"]-medias.loc[name,\"Total2\"])/sdesv.loc[name,\"Total2\"]\n",
    "    df_norm_inv.loc[group.index,[\"PLDDTZ-Score\"]] = (group[\"pLDDT\"]-medias.loc[name,\"pLDDT\"])/sdesv.loc[name,\"pLDDT\"]\n",
    "\n",
    "# Suma de Z-Score\n",
    "df_norm_inv.loc[:,\"Sum_Z\"]=df_norm_inv.loc[:,\"MCZ-Score\"]-df_norm_inv.loc[:,\"TEZ-Score\"]\n",
    "df_norm_inv.loc[:,\"Sum2_Z\"]=df_norm_inv.loc[:,\"MCZ-Score\"]-df_norm_inv.loc[:,\"TE2Z-Score\"]\n",
    "df_norm_inv.loc[:,\"Z-PLT\"]=df_norm_inv.loc[:,\"PLDDTZ-Score\"]-df_norm_inv.loc[:,\"TEZ-Score\"]\n",
    "df_norm_inv.loc[:,\"Z-PLT2\"]=df_norm_inv.loc[:,\"PLDDTZ-Score\"]-df_norm_inv.loc[:,\"TE2Z-Score\"]\n",
    "\n",
    "# Ranking Z-Score\n",
    "for name, group in grouped:\n",
    "    df_norm_inv.loc[group.index,\"Ranking_Z\"]=df_norm_inv.loc[group.index,\"Sum_Z\"].rank(ascending=False)\n",
    "    df_norm_inv.loc[group.index,\"Ranking2_Z\"]=df_norm_inv.loc[group.index,\"Sum2_Z\"].rank(ascending=False)\n",
    "    df_norm_inv.loc[group.index,\"Ranking_PLT\"]=df_norm_inv.loc[group.index,\"Z-PLT\"].rank(ascending=False)\n",
    "    df_norm_inv.loc[group.index,\"Ranking_PLT2\"]=df_norm_inv.loc[group.index,\"Z-PLT2\"].rank(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_inv.to_csv(directorio_csv + \"/df_norm_inv_\"+Target_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b31fb",
   "metadata": {},
   "source": [
    "#### 5.3 TOP100 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198fcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm=pd.read_csv(directorio_csv + \"/df_norm_\"+Target_name+\".csv\")\n",
    "df_norm_inv=pd.read_csv(directorio_csv + \"/df_norm_inv_\"+Target_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def obtener_archivos_csv(directorio_csv):\n",
    "    archivos_csv = [archivo for archivo in os.listdir(directorio_csv) if archivo.startswith('df_norm')]\n",
    "    return archivos_csv\n",
    "\n",
    "# Reemplaza 'ruta/del/directorio' con la ruta real de tu directorio\n",
    "archivos_csv = obtener_archivos_csv(directorio_csv)\n",
    "\n",
    "if archivos_csv:\n",
    "    for archivo_csv in archivos_csv:\n",
    "        print(archivo_csv)\n",
    "else:\n",
    "    print(\"No se encontraron archivos CSV en el directorio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de835d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ranking = [ \"Ranking2_Z\"]\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    a = pd.read_csv(directorio + archivo)\n",
    "    if len(archivo.split(\"_\")) > 3: \n",
    "\n",
    "        inv='_'+archivo.split(\"_\")[2]\n",
    "        print(inv)\n",
    "    else:\n",
    "        inv=''\n",
    "\n",
    "    for complejo in a[\"Complex\"].unique():\n",
    "        df_complejo = a[a[\"Complex\"] == complejo].copy()\n",
    "        \n",
    "        for Rank in Ranking:\n",
    "            if \"PLT\" in Rank:\n",
    "               df_filtrado = df_complejo[df_complejo[\"Version\"] == \"deepfold_v1\"].copy()\n",
    "     \n",
    "            else:\n",
    "                df_filtrado = df_complejo\n",
    "                #print(df_filtrado)\n",
    "            \n",
    "            # Filtrar primero por los top 100 según Rank\n",
    "            top100_preorden = df_filtrado.nsmallest(100, Rank)\n",
    "            \n",
    "            # Luego, ordenar por Rank si es necesario\n",
    "            top100_ordenado = top100_preorden.sort_values(by=Rank, ascending=True)\n",
    "             # Crear la carpeta si no existe\n",
    "            print (directorio , complejo , \"_\" ,Rank ,inv)\n",
    "            nueva_carpeta = directorio + complejo + \"_\" + Rank + inv\n",
    "            os.makedirs(nueva_carpeta, exist_ok=True)\n",
    "            \n",
    "            # Mover archivos especificados en la columna 'PATH'\n",
    "            for idx, fila in top100_ordenado.iterrows():\n",
    "                ruta_original = fila['PATH']\n",
    "                shutil.copy(ruta_original, nueva_carpeta)\n",
    "            \n",
    "            # Guardar a CSV\n",
    "            print(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.txt\")\n",
    "            top100_ordenado['Name'].to_csv(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.txt\", index=False, header=False)\n",
    "            top100_ordenado.to_csv(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.csv\", index=False)\n",
    "            \n",
    "        #print(complejo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcbabc",
   "metadata": {},
   "source": [
    "#### 5.4 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5037a7",
   "metadata": {},
   "source": [
    "To see diferent conformations (not used, too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2fa082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_archivo_ini(modelslist, RMSD_cutoff, receptor_mol, ligand_mol, filename='pyCluster_config.ini'):\n",
    "    import configparser\n",
    "    directorio = os.path.dirname(modelslist)\n",
    "    nombre_config = os.path.join(directorio, filename)\n",
    "    modelslist=os.path.basename(modelslist)\n",
    "    # Crear el objeto ConfigParser\n",
    "    config = configparser.ConfigParser()\n",
    "    # Agregar la sección 'clustering'\n",
    "    config['clustering'] = {\n",
    "        'modelslist': modelslist,\n",
    "        'RMSD_cutoff': RMSD_cutoff\n",
    "    }\n",
    "    # Agregar la sección 'receptor'\n",
    "    config['receptor'] = {\n",
    "        'mol': receptor_mol\n",
    "    }\n",
    "    # Agregar la sección 'ligand'\n",
    "    config['ligand'] = {\n",
    "        'mol': ligand_mol\n",
    "    }\n",
    "    # Escribir el archivo de configuración\n",
    "    with open(nombre_config, 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "    return os.path.basename(nombre_config)\n",
    "#cluster_list_files = [directorio +\"/\"+ complejo+\"_\"+nombre+\"/\"+ complejo+\"_\"+nombre+ \"_top100.txt\" for nombre in Ranking]\n",
    "cluster_list_files = [directorio +\"/\"+ Target_name+\"_\"+nombre+\"/\"+ Target_name+\"_\"+nombre+ \"_top100.txt\" for nombre in Ranking]\n",
    "\n",
    "# for cluster_list_file in cluster_list_files:\n",
    "#      #Ejecutar pydock4 pyCluster\n",
    "#      INI_FILE = crear_archivo_ini(cluster_list_file, 2, receptor_mol,ligand_mol)\n",
    "#      #INI_FILE = crear_archivo_ini(cluster_list_file, 4, receptor_mol,ligand_mol) #T266\n",
    "#      DIR_NAME = os.path.dirname(cluster_list_file)\n",
    "#      print (DIR_NAME)\n",
    "#      #subprocess.call(\"pydock4 \"+INI_FILE.strip(\".ini\")+\" pyCluster\", cwd=DIR_NAME, shell=True)\n",
    "#      #Generar los csv para Ranking clusterizados\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e101067",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_ordenado =pd.read_csv(directorio + complejo + \"_\" + Rank + \"/\"+ complejo + \"_\" + Rank + \"_top100.csv\")\n",
    "\n",
    "skip_clus=True\n",
    "if skip_clus:\n",
    "    df_to_send=df_norm\n",
    "\n",
    "### Uncomment if clusteing is needed.\n",
    "\n",
    "\n",
    "# clustered_list_file= pd.read_csv(DIR_NAME +\"/cluster_pyCluster_config.list\", header=None)\n",
    "\n",
    "# directorio + complejo + \"_\" + Rank \n",
    "# clustered_list_file.columns=['Name']\n",
    "# cols=top100_ordenado.columns\n",
    "# clustered_all_pydock = clustered_list_file.merge(top100_ordenado, on= 'Name')\n",
    "# # Reordenar las columnas según las columnas de top100_ordenado\n",
    "# clustered_all_pydock = clustered_all_pydock.reindex(columns=top100_ordenado.columns)\n",
    "\n",
    "# # Guardar el resultado en un archivo CSV\n",
    "# clustered_all_pydock.to_csv(DIR_NAME + \"/\" + Target_name + \"_cluster_pyCluster_config.csv\", index=False)\n",
    "\n",
    "# # Calcular la diferencia y agregarla como una nueva columna\n",
    "# clustered_all_pydock['Diferencia_R2_Z'] = clustered_all_pydock['Ranking2_Z'].diff(periods=-1) * -1\n",
    "\n",
    "# # Convertir las columnas seleccionadas a tipo numérico y manejar errores\n",
    "# cols_to_convert = ['Ranking_Z', 'Ranking2_Z', 'Ranking_PLT', 'Ranking_PLT2', 'Diferencia_R2_Z']\n",
    "# clustered_all_pydock[cols_to_convert] = clustered_all_pydock[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "\n",
    "# # Lógica para elegir el conjunto de datos\n",
    "# elegir_top100 = False\n",
    "\n",
    "# # Condición 1: Si hay más de un dato en clustered_all_pydock\n",
    "# if len(clustered_all_pydock) > 1:\n",
    "#     # Condición 2: Si entre los primeros 5 elementos de Diferencia_R2_Z hay alguno mayor que 10\n",
    "#     if (clustered_all_pydock['Diferencia_R2_Z'].head(5) > 10).any():\n",
    "#         elegir_top100 = True\n",
    "# else:\n",
    "#     elegir_top100 = True\n",
    "\n",
    "# # Seleccionar el DataFrame basado en las condiciones\n",
    "# if elegir_top100:\n",
    "#     df_to_send = top100_ordenado\n",
    "# else:\n",
    "#      # Seleccionar los datos de top100_ordenado que no están en clustered_all_pydock\n",
    "#     inverse_selection = top100_ordenado[~top100_ordenado['Name'].isin(clustered_all_pydock['Name'])]\n",
    "    \n",
    "#     # Concatenar clustered_all_pydock con la selección inversa\n",
    "#     df_to_send = pd.concat([clustered_all_pydock, inverse_selection], ignore_index=True)\n",
    "#     cols_to_convert = ['Ranking_Z', 'Ranking2_Z', 'Ranking_PLT', 'Ranking_PLT2', 'Diferencia_R2_Z']\n",
    "#     df_to_send[cols_to_convert] = df_to_send[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_carpeta = directorio + complejo + \"_\" + Rank + '_inv'\n",
    "result_df_inverted2= pd.read_csv(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.csv\")\n",
    "result_df_inverted2=result_df_inverted2.sort_values('Ranking_PLT2')\n",
    "if len(df_to_send)< 100: \n",
    "    df_to_send = pd.concat([df_to_send, result_df_inverted2], ignore_index=True)\n",
    "    df_to_send = df_to_send[:100]\n",
    "df_to_send"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39818ef9",
   "metadata": {},
   "source": [
    "#### 5.5 Copy to the To_send directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc58ee",
   "metadata": {},
   "source": [
    "- Scorers: to send minuscula\n",
    "- Predictors: to send en mayuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68caeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(to_send_dir,exist_ok=True)\n",
    "for file in df_to_send['PATH']:\n",
    "    shutil.copy(file,to_send_dir)\n",
    "df_to_send.to_csv(to_send_csv, index=False)\n",
    "df_to_send.to_csv(to_send_csv.replace('ene','csv'), index=False)\n",
    "df_to_send['Name'].to_csv(to_send_csv.replace('ene','txt'), index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d2988",
   "metadata": {},
   "source": [
    "### Extra target T264 T265\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fbcc3",
   "metadata": {},
   "source": [
    "Leemos un ficher con los RMSD calculados y selccionamos los modelos para cada target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64052e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSD_clusters_selection=pd.read_csv(to_send_dir + \"/\"+ \"RMSD_selection\",sep=\" \")\n",
    "# RMSD_clusters_selection = RMSD_clusters_selection.drop(index=0)\n",
    "# RMSD_clusters_selection =RMSD_clusters_selection.drop(columns=\"Unnamed: 5\")\n",
    "# RMSD_clusters_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5dc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(to_send_dir+'/T265',exist_ok=True)\n",
    "# filtered_model_names_T265 = RMSD_clusters_selection[(RMSD_clusters_selection.iloc[:, 1] < 8) | (RMSD_clusters_selection.iloc[:, 3] < 8)][\"Model_name\"]+'.pdb'\n",
    "# filtered_model_names_T265=filtered_model_names_T265.to_frame()\n",
    "# filtered_model_names_T265.to_csv(to_send_dir+'/T265'+'/T265_predictor_to_send.txt',index=False,header=None)\n",
    "# for file in filtered_model_names_T265['Model_name']:\n",
    "#     shutil.copy(to_send_dir+file,to_send_dir+'/T265')\n",
    "\n",
    "# os.makedirs(to_send_dir+'/T264',exist_ok=True)\n",
    "# filtered_model_names_T264 = RMSD_clusters_selection[(RMSD_clusters_selection.iloc[:, 2] < 8) | (RMSD_clusters_selection.iloc[:, 4] < 8)][\"Model_name\"]+'.pdb'\n",
    "# filtered_model_names_T264=filtered_model_names_T264.to_frame()\n",
    "# filtered_model_names_T264.to_csv(to_send_dir+'/T264'+'/T264_predictor_to_send.txt',index=False,header=None)\n",
    "# for file in filtered_model_names_T264['Model_name']:\n",
    "#     shutil.copy(to_send_dir+file,to_send_dir+'/T264')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb01ca",
   "metadata": {},
   "source": [
    "fold_t288_model_3.pdb fold_t288_model_0.pdb fold_t288_model_1.pdb T288_unrelaxed_rank_003_alphafold2_multimer_v2_model_1_seed_000.r19.pdb T288_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_001.r20.pdb T288_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_001.r19.pdb T288_unrelaxed_rank_009_alphafold2_multimer_v3_model_3_seed_000.r19.pdb T288_unrelaxed_rank_009_alphafold2_multimer_v3_model_3_seed_000.r18.pdb T288_unrelaxed_rank_009_alphafold2_multimer_v3_model_3_seed_000.r17.pdb T288_relaxed_rank_001_alphafold2_multimer_v3_model_1_seed_001.pdb T288_unrelaxed_rank_008_alphafold2_multimer_v3_model_2_seed_001.r6.pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7acee",
   "metadata": {},
   "source": [
    "T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r16.pdb T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r12.pdb T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r14.pdb T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r13.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r11.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r19.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r15.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r17.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r14.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r20.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r10.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r16.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r18.pdb T292_relaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r16.pdb T292_relaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r20.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r13.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r12.pdb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
