{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5bdfc94",
   "metadata": {},
   "source": [
    "# Alphafold models analysis main program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317f506",
   "metadata": {},
   "source": [
    "## Description of the materials and program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd66594",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3b048",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "# DataFrame Column Descriptions\n",
    "\n",
    "- **Ele**: Electrostatic energy of the complex. Measures the interaction between electric charges within the complex.\n",
    "- **Desolv**: Desolvation energy. Represents the energetic cost associated with desolvating individual molecules to form the complex.\n",
    "- **VDW**: Van der Waals energy. Measures the attractive and repulsive interactions between atoms that are not chemically bonded.\n",
    "- **Total**: Total energy of the complex. Sum of all energetic contributions (Electrostatic + Desolvation + 0.1 Van der Waals).\n",
    "- **Name**: Name of the object or element. Used to identify and merge data from different datasets.\n",
    "- **PATH**: File path associated with the object. Stores the locations of the files corresponding to each object for additional input/output operations.\n",
    "- **Complex**: Name or identifier of the studied complex.\n",
    "- **State**: State of the complex (e.g., native, mutated, etc.).\n",
    "- **Model**: Specific model used in the analysis.\n",
    "- **Rank**: Ranking of the model or complex based on a specific criterion.\n",
    "- **Version**: Version of the model or software used in the analysis.\n",
    "- **Recycle**: Number of times the model has been recycled or reused in iterations.\n",
    "- **Seed**: Seed value used by AlphaFold2.\n",
    "- **Unstructured_count**: Number of unstructured regions in the complex.\n",
    "- **Max_unstructured_region**: Size of the largest unstructured region.\n",
    "- **Total_clashes**: Total number of atomic clashes within the complex.\n",
    "- **Clashes_chain_A**: Number of clashes in chain A.\n",
    "- **Clashes_chain_B**: Number of clashes in chain B. _There may be more chains._\n",
    "- **Low_B_factors_chain_A**: Percentage of residues with pLDDT below 50 in chain A.\n",
    "- **Low_B_factors_chain_B**: Percentage of residues with pLDDT below 50 in chain B. _There may be more chains._\n",
    "- **Knots**: Number of knots present in the structure.\n",
    "- **pLDDT**: Predicted Local Distance Difference Test. Measures the quality of the local structural prediction.\n",
    "- **pTM**: Predicted Template Modeling. Measures the quality of the global structural prediction based on template modeling.\n",
    "- **ipTM**: Interface Predicted Template Modeling. Measures the quality of the structural prediction at interfaces.\n",
    "- **tol**: Tolerance of the model or simulation.\n",
    "- **Model_confidence**: Confidence in the predictive model. Calculated as ipTM\\*0.8 + pTM\\*0.3.\n",
    "- **Total2**: Unweighted total energy from pyDock (Electrostatic + Desolvation + Van der Waals).\n",
    "- **MCZ-Score**: Model Confidence Z-score.\n",
    "- **PLDDTZ-Score**: pLDDT Z-score.\n",
    "- **TEZ-Score**: Z-score calculated from Total.\n",
    "- **TE2Z-Score**: Z-score calculated from Total2.\n",
    "- **Sum_Z**: Sum of the Z-scores for Model Confidence and Total.\n",
    "- **Sum2_Z**: Sum of the Z-scores for Model Confidence and Total2.\n",
    "- **Z-PLT**: Sum of the Z-scores for pLDDT and Total.\n",
    "- **Z-PLT2**: Sum of the Z-scores for pLDDT and Total2.\n",
    "- **Ranking_Z**: Ranking based on Sum_Z.\n",
    "- **Ranking2_Z**: Ranking based on Sum2_Z.\n",
    "- **Ranking_PLT**: Ranking based on the Z-PLT criterion.\n",
    "- **Ranking_PLT2**: Ranking based on the Z-PLT2 criterion.\n",
    "- **Diferencia_R2_Z**: Difference between the current ranking and the next in the Ranking2_Z column. Indicates the cluster size.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23459f46",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "   \n",
    "This jupyter notebook is created to perform a analysis of complexes generated by different versions of Alphafold. There are 5 versions of Alpahafold:\n",
    "- Aplhafold2: is the standart versions developed by Google Deepmind\n",
    "- Alphafold multimer v1 (v1): a variation of Alphafold2 to model complexeses more properly\n",
    "- Alphafold multimer v2 (v2):\n",
    "- Alphafold multimer v3 (v3):\n",
    "\n",
    "- RMSD:  stands for Root Mean Square Deviation, and it is a measure used in structural biology to assess the similarity or deviation between two or more protein or molecular structures. It quantifies the average distance between the corresponding atoms of two superimposed structures. So it works as indication of similarity, the lower the RMSD the higher likelihood of the two structures.This would be essential to see if AM is able to replicate the sctructure provided by the cristal of the PDB_databank. There are different types of RMSD...\n",
    "\n",
    "- Total energy: Pydock4\n",
    "\n",
    "- Model confidence: indicates the reliability of the model generated. When Alphafold generates a model it also assigns a predicted local difference distance test score (pLDDT, corresponding to local structural accuracy), predicted TM-score (pTM,corresponding to overall topological accuracy), and an interface pTM score (ipTM) which is used in conjunction with pTM to compute model scores. The model coonfidence is calculated by: $0.8 \\cdot iptm + 0.2 \\cdot ptm$. Also the AI determines the tol which are used to determine if the program should do another recyle or not depending of the previous generation, if the model doesn't improve much it will stop and perform a relaxation of the structure using AMBER force field.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af69411",
   "metadata": {},
   "source": [
    "### Description of the files and folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25140c",
   "metadata": {},
   "source": [
    "#### Complex folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caec483",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "The folder in which the rest files are stored are named by the complex, composed by the name of the cristal in the PDB bank followed by the chains used to do the complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d2f0f",
   "metadata": {},
   "source": [
    "#### PDBS files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526df74",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1; text-align: justify;\">\n",
    "\n",
    "Indicates the information of of the protein structure. The names of the PDBs generated by Aplhafolds are composed by:  complex,state,rank,version of Alphafold, model and recycle (except cristals,\"ranked\" pdbs,Seed_0).<br><br>\n",
    "\n",
    "- Complex: the name of the complex registered in the PDB bank, it  is composed by letters and numbers.<br><br>\n",
    "- States:\n",
    "  - unrelaxed: are crude structures provided by Alphafold in which it does it iterative proccess .\n",
    "  \n",
    "  - relaxed: The last structure recycled relaxed using AMBER force field. <br><br>\n",
    "\n",
    "- Version: the five versions described in the introduction(v1,v2,v3) in the future can be more.<br><br>\n",
    "\n",
    "\n",
    "- Model:\n",
    "\n",
    "  - Models in Alphafold2: generates five predictions from the same seed, are named as \"model_\" followed by a number.\n",
    "  \n",
    "\n",
    "    - \"ranked_\" folled by a number: indicates in which position in the rank are the relaxed models according to the scores that alphafold assigns. Their name is entirely \"ranked\" it has no more data in it.\n",
    "\n",
    "\n",
    "    - \"pred_\" followed by a number: identifies a model generated by the same seed, but with minor differences.<br><br>\n",
    "    \n",
    "  \n",
    "  - Model in the versions of AM (v1,v2,v3,v3_short): the models are generate models 5 model from differents seeds and then it iterates the resolution of the structure until the tol variable surpass a threshold in which alphafold stop modeling ot reaches the recycle of 20.<br><br>\n",
    "\n",
    "- Recycle: only for non-Alphafold2 predictions (at te moment).\n",
    "  \n",
    "  - \"r_\" followed by number : indicates the recycle of the model.\n",
    "\n",
    "\n",
    "  - \"Seed_0\": is the same from recycle 20 that will be relaxed.<br><br>\n",
    "  \n",
    "- Rank folllowed by a number : it indicates which model of the five generated is best according to the highest score obtained in the last recyle, only in Alphafold2.<br><br>\n",
    "\n",
    "\n",
    "- Examples of names:\n",
    "\n",
    "\n",
    "  - unrelaxed_rank_001_alphafold2_multimer_v2_model_4_seed_000_r9.pdb (standart name in AM versions).\n",
    "\n",
    "\n",
    "  - relaxed_model_4_multimer_v2_pred_1.pdb (standart name in Alphafold2 versions).\n",
    "\n",
    "\n",
    "  - 3BT1.pdb (crystal).\n",
    "\n",
    "\n",
    "  - ranked_0.pdb (relaxed and ranked in Alphafold2).\n",
    "\n",
    "  \n",
    "  - unrelaxed_rank_001_alphafold2_multimer_v3_model_2_seed_000_r0 ( Seed_0 example).\n",
    "\n",
    "   \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06766a26",
   "metadata": {},
   "source": [
    "#### Json files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dad52",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "   \n",
    "\n",
    "regarding to model confidence and the time used to create the model\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d3f39",
   "metadata": {},
   "source": [
    "#### Log.txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a44d6",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1; text-align: justify;\">\n",
    "   \n",
    "It gathers infromation about the execution of alphafold, the most relevant information is:\n",
    "\n",
    "- Timestamps: The file starts with timestamps indicating when each event occurred. These timestamps are in the format \"YYYY-MM-DD HH:MM:SS,sss\" (Year, Month, Day, Hour, Minute, Second, Milliseconds).\n",
    "\n",
    "- Information about the software: The first few entries provide information about the software version (ColabFold 1.5.2).\n",
    "\n",
    "- Recycle iterations: The log then proceeds to provide information about the iterative process of protein structure prediction. It mentions recycling and various metrics such as \"pLDDT,\" \"pTM,\" \"ipTM,\" and \"tol\" for each recycle step.\n",
    "\n",
    "- Model ranking: The final section ranks the models based on the \"multimer\" metric, and it mentions the relaxation times for each model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328da18",
   "metadata": {},
   "source": [
    "### Description of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011bf83",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The notebook is constructuted to gather the information of the standarized names and the exceptions mentioned (see examples). If the names of the folders, the pdbs, log.txt or outputs of pydock4 are severly changed, this notebook could not work as intented.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab0867",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "   \n",
    "The analysis in the main program is divided in 6 sections:\n",
    "\n",
    "1. Libraries and initial values: It loads the libraries are needed and gather the names of folders to do later itartions folder by folder. \n",
    "2. Running pydock4: to calculate the RMSD according to CAPRI and the bind energy.\n",
    "3. Contruction of the dataframes: the outputs of pydock4 have to be cured to transform them into a dataframe. Also there is a Log.txt information retrieving in to a dataframe.  \n",
    "4. Final Fusion and adjustments: due to some pecularities of the versions some data have to be renamed, modified and/or remove to harmonized the data.\n",
    "\n",
    "There are two classes of folders. Ones have the pdb from Alphafold2 and the other are obtained from AlphaFold-multimers. The difference between them is how the information about the model confidence is stored, the ones from Alphafold2 have their model confidence stored in json archives and the ones from AM have in the log.txt. This implies a different aproach of gathering this data.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a7756",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253187e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "066a4adc",
   "metadata": {},
   "source": [
    "### 0. Paths and selected molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64440d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "Target_name=\"T309\"\n",
    "directorio=\"/home/luis/CAPRI_R57/\"+Target_name+\"/Predictors/AF_MODELS/COMPLEX/\"\n",
    "directorio_csv= \"/home/luis/CAPRI_R57/\"+Target_name+\"/Predictors/AF_MODELS/COMPLEX/\"# This is the the directory of the folder that will gather the outputs\n",
    "to_send_dir=\"/home/luis/CAPRI_R57/\"+Target_name+\"/Predictors/To_send/\"\n",
    "to_send_csv =\"/home/luis/CAPRI_R57/\"+Target_name+\"/Predictors/To_send/\"+Target_name+\"_predictor_to_send.ene\"\n",
    "\n",
    "# Target_name=\"T254\"\n",
    "# directorio=\"/home/luis/CAPRI_R57/T254/Predictors/Superposition_models_T255_new/\"\n",
    "# directorio_csv= \"//home/luis/CAPRI_R57/T254/Predictors/Superposition_models_T255_new/\"# This is the the directory of the folder that will gather the outputs\n",
    "\n",
    "# Target_name=\"T272\"\n",
    "# directorio=\"/home/luis/CAPRI_R57/T272/Predictors/SUPERPOSITION_MODELS/\"\n",
    "# directorio_csv= \"/home/luis/CAPRI_R57/T272/Predictors/SUPERPOSITION_MODELS/\"# This is the the directory of the folder that will gather the outputs\n",
    "\n",
    "#Clustering\n",
    "#receptor_mol,ligand_mol =[\"A\",\"B\"] #T236\n",
    "#receptor_mol,ligand_mol =[\"A,B\",\"C\"] #T238\n",
    "#receptor_mol,ligand_mol =[\"A,B,C,D\"],[\"E\"] #240\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T242\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T244\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T248\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T248\n",
    "#receptor_mol,ligand_mol =[\"A\",\"B\"],[\"C\"] #T250/T252\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T254/T255 por simertria solo cogemos dos cadenas\n",
    "#receptor_mol,ligand_mol =[\"A\"],[\"B\"] #T262 por simertria solo cogemos dos cadenas\n",
    "# receptor_mol,ligand_mol =[\"B,C\"],[\"A\"] #T266 Antibody\n",
    "# receptor_mol,ligand_mol =[\"B\"],[\"I\",\"K\"] #T264-T265 Protein_DNA\n",
    "# receptor_mol,ligand_mol =[\"A\",\"C,E\"],[\"B\",\"K,F\"] #T280\n",
    "#receptor_mol,ligand_mol =[\"B,C\"],[\"A\"] #T284\n",
    "#receptor_mol,ligand_mol =[\"B,C\"],[\"A\"], [\"A,C\"],[\"B\"], [\"B,A\"],[\"C\"] # T288\n",
    "#receptor_mol,ligand_mol =# T290\n",
    "\n",
    "receptor_mol,ligand_mol =[\"A,B,C\"],[\"D,E,F\"] #T290\n",
    "#receptor_mol,ligand_mol =[\"A,B,C\"],[\"D,E,F,G,H,I\"] #T292\n",
    "\n",
    "#receptor_mol,ligand_mol =[\"B,C\"],[\"A\"] #T266 Antibody\n",
    "#receptor_mol,ligand_mol =[\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],[\"A\"] #T272 Antibody\n",
    "\n",
    "\n",
    "print(directorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24106a8b",
   "metadata": {},
   "source": [
    "### 0.1. Model Relaxation OpenMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash -s \"$directorio/../\" \"$Target_name\"\n",
    "#cd $1\n",
    "#for i in `find  -name ''${2}'_*.r*.pdb'`;do echo \"python relax_v2.py -model_name $i -output_model_name $i\";done  > relax_greasy.txt\n",
    "#export GREASY_NWORKERS=1\n",
    "#greasy relax_greasy.txt \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac5973d1",
   "metadata": {},
   "source": [
    "### 1. Libraries and initial values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b8dc8d9",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\"> The following  libraries  are used to treat the data adn ploting:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File manegement\n",
    "import os, zipfile \n",
    "import re \n",
    "import shutil\n",
    "\n",
    "# Data manegement\n",
    "import pandas as pd # used to manage dataframes\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from Bio import PDB\n",
    "from Bio.PDB import MMCIFParser, PDBIO, DSSP, NeighborSearch,Superimposer,PDBParser\n",
    "from Bio.Align import PairwiseAligner\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import warnings\n",
    "# Subprocess to calling bash\n",
    "import subprocess # used to call bash and running external programs like pydock4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea27cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Parando la ejecución aquí.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9146794",
   "metadata": {},
   "source": [
    "### 1.1 Preprocess AlphaFold3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncompress the AplhaFold3 Job.\n",
    "def descomprimir_archivo(zip_path, directorio_destino):\n",
    "    \"\"\"\n",
    "    Descomprime un archivo ZIP en el directorio especificado.\n",
    "\n",
    "    Parámetros:\n",
    "    zip_path (str): Ruta del archivo ZIP.\n",
    "    directorio_destino (str): Ruta del directorio donde se colocarán los archivos descomprimidos.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que el directorio destino existe, si no, crearlo\n",
    "    if not os.path.exists(directorio_destino):\n",
    "        os.makedirs(directorio_destino)\n",
    "\n",
    "    # Abrir el archivo ZIP en modo de lectura\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extraer todos los archivos en el directorio especificado\n",
    "        zip_ref.extractall(directorio_destino)\n",
    "patron = r'(.*(\\d+)\\.zip$)'\n",
    "patron_CIF = r'(.*(\\d+)\\.cif$)'\n",
    "\n",
    "zipfiles = [os.path.abspath(os.path.join(directorio, archivo)) for archivo in os.listdir(directorio) if re.match(patron, archivo)]\n",
    "print(zipfiles)\n",
    "for zipfille in zipfiles:\n",
    "    print(zipfille)\n",
    "    descomprimir_archivo(zipfille,zipfille.rstrip('.zip'))\n",
    "\n",
    "CIF_files = [os.path.abspath(os.path.join(directorio, archivo)) for archivo in os.listdir(zipfille.rstrip('.zip')) if re.match(patron_CIF, archivo)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert CIF files to PDB files of the AplhaFold3 Job.\n",
    "def convert_cif_to_pdb(cif_file, pdb_file):\n",
    "    \"\"\"\n",
    "    Convert a CIF file to a PDB file using Biopython.\n",
    "\n",
    "    Parameters:\n",
    "    cif_file (str): Path to the input CIF file.\n",
    "    pdb_file (str): Path to the output PDB file.\n",
    "    \"\"\"\n",
    "    parser = MMCIFParser()\n",
    "    structure = parser.get_structure('ID', cif_file)\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(pdb_file)\n",
    "\n",
    "patron_CIF = r'(.*(\\d+)\\.cif$)'\n",
    "for zipfille in zipfiles:\n",
    "    CIF_files = [os.path.join(zipfille.rstrip('.zip'),archivo) for archivo in os.listdir(zipfille.rstrip('.zip')) if re.match(patron_CIF, archivo)]\n",
    "    #print(CIF_files)\n",
    "    for CIF_file in CIF_files:\n",
    "        #print(CIF_file)\n",
    "        convert_cif_to_pdb(CIF_file, CIF_file.replace('.cif','.pdb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Json\n",
    "import os\n",
    "import json\n",
    "\n",
    "def leer_json_extract_vars(directorio, claves):\n",
    "    \"\"\"\n",
    "    Lee archivos JSON en un directorio específico y extrae las variables especificadas.\n",
    "    \n",
    "    Parámetros:\n",
    "    directorio (str): Ruta al directorio que contiene los archivos JSON.\n",
    "    claves (list): Lista de claves a extraer de los archivos JSON.\n",
    "    \n",
    "    Retorna:\n",
    "    dict: Diccionario con nombres de archivo y sus variables extraídas.\n",
    "    \"\"\"\n",
    "    resultados = {}  # Diccionario para almacenar los resultados\n",
    "\n",
    "    # Recorrer todos los archivos en el directorio\n",
    "    pattern_json = re.compile(r\"summary_confidences_\\w\\.json$\")\n",
    "    for archivo in os.listdir(directorio):\n",
    "        if pattern_json.search(archivo):  # Asegurarse de que es un archivo JSON\n",
    "            ruta_completa = os.path.join(directorio, archivo)\n",
    "            with open(ruta_completa, 'r') as f:\n",
    "                data = json.load(f)  # Cargar el contenido JSON\n",
    "                # Extraer las variables especificadas\n",
    "                valores_extraidos = {clave: data.get(clave, None) for clave in claves}\n",
    "                \n",
    "                # Almacenar los resultados\n",
    "                resultados[archivo] = valores_extraidos\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Usar la función\n",
    "claves_a_extraer = ['ptm', 'iptm']  # Añadir aquí cualquier clave que necesites\n",
    "for zipfille in zipfiles:\n",
    "    log_folder = zipfille.rstrip('.zip')\n",
    "    resultados = leer_json_extract_vars(zipfille.rstrip('.zip'), claves_a_extraer)\n",
    "    with open(os.path.join(log_folder,'log.txt'), 'w') as file:\n",
    "        for archivo, vars in resultados.items():\n",
    "            # Formatear nombre del archivo y eliminar partes no deseadas\n",
    "            nombre_archivo_formateado = archivo.replace('summary_confidences', 'model').rstrip('.json')\n",
    "            # Crear una cadena de texto con los pares clave=valor\n",
    "            vars_text = ' '.join([f\"{key.replace('tm','TM')}={value}\" for key, value in vars.items()])\n",
    "            file.write(f\"{nombre_archivo_formateado} {vars_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "1050/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Parando la ejecución aquí.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adae3cad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>   Calculate the ByEnergy before the following step.\n",
    "   Now we are going to gather the working directories where the archive are located, then generate a list with the rutes of the archives to perform iteration and automatitation of the analysis..\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directorio_csv):\n",
    "    os.makedirs(directorio_csv)\n",
    "\n",
    "# Folders of all models\n",
    "carpetas = [nombre for nombre in os.listdir(directorio) if os.path.isdir(os.path.join(directorio, nombre))]\n",
    "\n",
    "#PDB files of the folders and the way we will \n",
    "archivos_pdb=[]\n",
    "patron = r'(.*(\\d+)\\.pdb$)'\n",
    "#patron = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.pdb' # T272\n",
    "datos_carpeta={}\n",
    "for carpeta in carpetas:  \n",
    "        patron =\"(\"+carpeta[0:4]+ \".pdb)|\" +patron\n",
    "        direccion = directorio + \"/\" + carpeta + \"/\"\n",
    "        pdbs=[os.path.abspath(os.path.join(direccion, archivo)) for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "        datos_carpeta={**datos_carpeta,**{carpeta:len(pdbs)}}\n",
    "        archivos_pdb.extend(pdbs)\n",
    "      \n",
    "# Folders runned by colab_alphafold\n",
    "carpetas_colab=[] \n",
    "for carpeta in carpetas:\n",
    "    logic_fold= carpeta.count(\"am\")\n",
    "    if(logic_fold>0):\n",
    "        carpetas_colab.append(carpeta)\n",
    "\n",
    "# PDB files of archives with colabfold\n",
    "archivos_colab=[]\n",
    "for carpeta in carpetas_colab:  \n",
    "        pdbs=[os.path.abspath(os.path.join(direccion, archivo)) for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "        narchivos=len(pdbs)\n",
    "        archivos_colab.extend(pdbs)\n",
    "\n",
    "# Name of cristals\n",
    "cristales= [nombre[0:4] for nombre in os.listdir(directorio) if os.path.isdir(os.path.join(directorio, nombre))]\n",
    "cristales= list(set(cristales))\n",
    "\n",
    "patron_cristal= \".pdb)|(\".join(cristales)\n",
    "patron_cristal= \"(\"+patron_cristal+\".pdb)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35b1ce",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "   The following block shows how many pdb models are in the folder\n",
    "   \n",
    "   </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numero_pdbs_by_dir(directorio):  \n",
    "    x=1\n",
    "    n_archivos=0\n",
    "    for carpeta in carpetas:\n",
    "        # Accedemos a cada una de ellas y ponemos en un documento lista la dirección de cada uno de los .pdb\n",
    "        direccion = directorio + \"/\" + carpeta + \"/\"\n",
    "        archivos_pdb = [archivo for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "        print(x,carpeta,len(archivos_pdb))\n",
    "        n_archivos=n_archivos+len(archivos_pdb) \n",
    "        x=x+1\n",
    "    return (n_archivos)\n",
    "print(numero_pdbs_by_dir(directorio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ffcd45",
   "metadata": {},
   "source": [
    "### 3. Data Frame creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f7000",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">\n",
    "In this section the information about ene and capri archives will be retrieve in their respective dataframes.\t\n",
    "\t<br>\n",
    "\n",
    "\n",
    "1. The CAPRI dataframe consists on:\n",
    "    - Name: name of the pdb\n",
    "    - Complex: name of the complex\n",
    "    - State: relaxed or unrelaxed\n",
    "    - Version: version of alphafold used.\n",
    "    - Model: name of the model\n",
    "    - Rank: rank according to model confidence\n",
    "    - Recycle: recycle of the model\n",
    "    - Conf\n",
    "    - l_RMSD\n",
    "    - i_RMSD\n",
    "    - FNC\n",
    "    - CAPRI\t\n",
    "    <br>\t<br>\t\t\n",
    "2. The Bind Energy dataframes consists on:\n",
    "    - Name: name of the pdb\n",
    "    - Complex: name of the complex\n",
    "    - State: relaxed or unrelaxed\n",
    "    - Version: version of alphafold used.\n",
    "    - Model: name of the model\n",
    "    - Rank: rank according to model confidence\n",
    "    - Recycle: recycle of the model\n",
    "    - Conf\t\n",
    "    - Ele\n",
    "    - Desolv\n",
    "    - VDW\n",
    "    - Total\n",
    "    - RMSD\n",
    "    - RANK\n",
    "3. The log.txt dataframe consist of:\n",
    "    - Complex: name of the complex\n",
    "    - State: relaxed or unrelaxed\n",
    "    - Version: version of alphafold used.\n",
    "    - Model: name of the model\n",
    "    - Rank: rank according to model confidence\n",
    "    - Recycle: recycle of the model\n",
    "    - pLDDT:\n",
    "    - pTM:\n",
    "    - ipTM:\n",
    "    - tol:\n",
    "    - Model_confindence:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b5c96a1",
   "metadata": {},
   "source": [
    "#### 3.2 Bind energy dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dead36",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\"> First,calculate the energy. Gathering the information from the .ene files.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d21669",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid red; padding: 10px; background-color: #ffdddd;\">\n",
    "  <strong>ACTUALIZACIÓN DEL CÓDIGO:</strong> He puesto para que si <code>./sum_ene_multy_bindEy_new.sh</code> no devuelve la cabecera pues haga la tabla cogiendo la primera fila. Issue detectado en el target <code>T284</code> en el que solo se generaba de la suma un ene sin cabeceras.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee20ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7126b48e",
   "metadata": {},
   "source": [
    "Actualizacion 2: Se ha comentado lo que escribio luis en un primer momento, mucho texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfab3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Inicializar un DataFrame vacío para almacenar los resultados finales\n",
    "# total_df=pd.DataFrame()\n",
    "# resultado_df = pd.DataFrame()\n",
    "# extension_final = len(\".ene\")\n",
    "# patron = r\".*\\d\\.ene$\"\n",
    "# #patron  = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.ene'\n",
    "# #print(carpetas)\n",
    "# for carpeta in carpetas:\n",
    "#     direccion = directorio + \"/\" + carpeta + \"/\"\n",
    "#     archivos_ene = [archivo for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "#     resultado_df = pd.DataFrame()\n",
    "    \n",
    "#     for archivo in archivos_ene:\n",
    "#         print(os.path.join(direccion, archivo))\n",
    "#         # Inicializar la tabla como una lista vacía\n",
    "#         tabla = []\n",
    "        \n",
    "#         # Leer el archivo y procesar cada línea\n",
    "#         with open(os.path.join(direccion, archivo), 'r') as file:\n",
    "#             # Leer la primera línea como nombres de columnas\n",
    "#             column_names = file.readline().strip().split()\n",
    "                        \n",
    "#             lineas = file.readlines()\n",
    "#             num_lineas = len(lineas)\n",
    "#             # Asegurarte de que haya una columna adicional en el encabezado\n",
    "#             if len(column_names) < 5:\n",
    "#                 column_names.append(\"ColumnaVacia\")\n",
    "\n",
    "#             # Ignorar la segunda línea\n",
    "#             file.readline()\n",
    "\n",
    "#             # Agregar los nombres de las columnas a la tabla\n",
    "#             tabla.append(column_names)\n",
    "\n",
    "#             # Leer y agregar los valores de la tercera línea en adelante\n",
    "#             for linea in file:\n",
    "#                 valores = linea.strip().split()\n",
    "#                 # Asegurarte de que haya una columna adicional en los datos\n",
    "#                 if len(valores) < 5:\n",
    "#                     valores.append(np.nan)\n",
    "#                 tabla.append(valores)\n",
    "#         print(num_lineas)\n",
    "#         if num_lineas==0:\n",
    "#             df = pd.DataFrame([tabla[0]], columns=[\"Conf\",\"Ele\",\"Desolv\", \"VDW\",\"Total\",\"RANK\"])\n",
    "#         else:\n",
    "#             df = pd.DataFrame(tabla[1:], columns=tabla[0])\n",
    "#         df[\"Name\"] = archivo[:-extension_final]\n",
    "#         df[\"PATH\"] = os.path.join(direccion, archivo).rstrip(\".ene\")+\".pdb\"\n",
    "#         print(df)\n",
    "#         #Concatenar el DataFrame actual con el resultado_df\n",
    "      \n",
    "#         resultado_df = pd.concat([resultado_df, df], ignore_index=True)        \n",
    "#     if carpeta.startswith('fold'):\n",
    "#         resultado_df[\"Complex\"]=carpeta.split('_')[1].upper()\n",
    "#     else:\n",
    "#         resultado_df[\"Complex\"]=carpeta[0:4]\n",
    "#     total_df=pd.concat([total_df,resultado_df], ignore_index=True)\n",
    "#     # print(carpeta)\n",
    "\n",
    "# total_df.to_csv(directorio_csv + \"pydock4_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un DataFrame vacío para almacenar los resultados finales\n",
    "total_df=pd.DataFrame()\n",
    "resultado_df = pd.DataFrame()\n",
    "extension_final = len(\".ene\")\n",
    "patron = r\".*\\d\\.ene$\"\n",
    "#patron  = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.ene'\n",
    "#print(carpetas)\n",
    "for carpeta in carpetas:\n",
    "    direccion = os.path.join(directorio, carpeta )\n",
    "    archivos_ene = [archivo for archivo in os.listdir(direccion) if re.match(patron, archivo)]\n",
    "    resultado_df = pd.DataFrame()\n",
    "    \n",
    "    for archivo in archivos_ene:\n",
    "        print(os.path.join(direccion, archivo))\n",
    "        # Inicializar la tabla como una lista vacía\n",
    "        tabla = []\n",
    "        df = pd.read_csv(os.path.join(direccion, archivo), sep='\\s+', skiprows=[1])\n",
    "        df[\"Name\"] = archivo[:-extension_final]\n",
    "        df[\"PATH\"] = os.path.join(direccion, archivo).rstrip(\".ene\")+\".pdb\"\n",
    "        print(df)\n",
    "        #Concatenar el DataFrame actual con el resultado_df\n",
    "      \n",
    "        resultado_df = pd.concat([resultado_df, df], ignore_index=True)        \n",
    "    if carpeta.startswith('fold'):\n",
    "        resultado_df[\"Complex\"]=carpeta.split('_')[1].upper()\n",
    "    else:\n",
    "        resultado_df[\"Complex\"]=carpeta[0:4]\n",
    "    total_df=pd.concat([total_df,resultado_df], ignore_index=True)\n",
    "    # print(carpeta)\n",
    "\n",
    "total_df.to_csv(directorio_csv + \"pydock4_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ca056",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e088f37",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\"> Asignation of the data related to de name of the model: state, model, rank, version and recyle  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (directorio_csv)\n",
    "df = pd.read_csv(directorio_csv + \"pydock4_raw.csv\", sep=r'\\t|,')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data_frame\n",
    "df = pd.read_csv(directorio_csv + \"pydock4_raw.csv\", sep=r'\\t|,')\n",
    "\n",
    "#information to retrieve\n",
    "state_pattern = re.compile(r'.nrelaxed')\n",
    "version_pattern = re.compile(r\"((deepfold|alphafold2_multimer)_v\\d+)_model\")\n",
    "model_pattern = re.compile(r'model_(\\d+)')\n",
    "rank_pattern = re.compile(r'(rank_(\\d+))|(pred_\\d+)|(ranked_.*)')\n",
    "recycle_pattern = re.compile(r'(_|.)r(\\d{1,})')\n",
    "#seed_pattern = re.compile(r'seed_([0-9]+)\\.')\n",
    "#seed_pattern = re.compile(r'seed_([\\d]+)\\.')\n",
    "seed_pattern = re.compile(r'seed_([0-9]+)(?:\\.|$)')\n",
    "\n",
    "# Defining empty list where the data from the file name will be gather\n",
    "state=[]\n",
    "model=[]\n",
    "version = []\n",
    "recycle = []\n",
    "rank=[]\n",
    "seed=[]\n",
    "# Loop to gather the information entry by entry\n",
    "for linea in (df[\"Name\"].tolist()):\n",
    "    #State relaxed, unrelaxed\n",
    "    match = state_pattern.search(linea)\n",
    "    if match:\n",
    "        state.append(match.group(0))\n",
    "    else:\n",
    "        state.append(\"relaxed\")\n",
    "    \n",
    "    # Model\n",
    "    match = model_pattern.search(linea)\n",
    "    if match:   \n",
    "        model.append(match.group(1)) \n",
    "    else:\n",
    "        model.append(\"cristal\")   \n",
    "    \n",
    "    #Rank\n",
    "    match = rank_pattern.search(linea)\n",
    "    if match:   \n",
    "        rank.append(match.group(0)) \n",
    "    else:\n",
    "        rank.append(\"unrank\")\n",
    "    \n",
    "    #Version \n",
    "    match = version_pattern.search(linea)\n",
    "    if match: \n",
    "        version.append(match.group(1))\n",
    "    else:\n",
    "        version.append(\"cristal\")\n",
    "    \n",
    "    # Recycle\n",
    "    match = recycle_pattern.search(linea)\n",
    "    if match:\n",
    "        recycle.append(match.group(0)[2:])\n",
    "    else:\n",
    "        recycle.append(\"Seed_0\")          \n",
    "    #Seed\n",
    "    match = seed_pattern.search(linea)\n",
    "    if match:\n",
    "        seed.append(match.group(1))\n",
    "    else:\n",
    "        seed.append(\"-\")\n",
    "\n",
    "# Adding the entries to the dataframe\n",
    "df[\"State\"]=state\n",
    "df[\"Model\"]=model\n",
    "df[\"Rank\"]=rank\n",
    "df[\"Version\"]=version\n",
    "df[\"Recycle\"]=recycle\n",
    "df[\"Seed\"]=seed\n",
    "\n",
    "# Añadimos informacion de los cristales\n",
    "df.loc[df[\"Rank\"] == \"unrank\", \"Version\"] = \"alphafold3\"\n",
    "df.loc[(df[\"Model\"] == \"cristal\") & (df[\"Rank\"] == \"unrank\"), [\"Rank\", \"Recycle\", \"State\", \"Version\"]] = \"cristal\"\n",
    "lista_valores = [\"pred_0\", \"pred_1\", \"pred_2\", \"pred_3\", \"pred_4\", \"pred_5\"]\n",
    "df.loc[df[\"Rank\"].isin(lista_valores), \"Version\"] = \"Alphafold2\"\n",
    "\n",
    "# Añadimos informacion del estadio\n",
    "#df.loc[df[\"State\"]==\"relaxed\",\"Recycle\"]=\"relaxed\"\n",
    "df['Name']= df['Name']+\".pdb\"\n",
    "\n",
    "# Añadimos informacion de los ranked\n",
    "df.loc[(df[\"Model\"] == \"cristal\") & (df[\"Rank\"] != \"cristal\"),  \"Version\"] = \"Alphafold2\"\n",
    "df.loc[(df[\"Model\"] == \"cristal\") & (df[\"Rank\"] != \"cristal\"), [\"Model\",  \"Recycle\"]] = \"ranked\"\n",
    "\n",
    "# Eliminamos duplicaciones\n",
    "#df=df.drop_duplicates(subset=[\"Name\",\"State\",\"Complex\"],keep=\"first\")\n",
    "df_pydock=df\n",
    "df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17761ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The models were all relaxed\n",
    "df_pydock[\"State\"]=\"relaxed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b692fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock.to_csv(directorio_csv+'/pydock4_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Calculation of additional parameters ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56064a",
   "metadata": {},
   "source": [
    "Calcula:\n",
    "- Zonas estructuradas/desestructuradas y el numero total de aa en la desetructuracion\n",
    "- choques entre cadenas(se coge el total, pero se puede ver la expecificación zonas)\n",
    "- Simetria de cadenas iguales (Rise, grados sexagesimales)\n",
    "- Deteccion de nudos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90409965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def add_cryst1_record(pdb_file):\n",
    "    cryst1_line = \"CRYST1   90.000   90.000   90.000  90.00  90.00  90.00 P 1           1\\n\"\n",
    "    with open(pdb_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    if not any(line.startswith('CRYST1') for line in lines):\n",
    "        with open(pdb_file, 'w') as file:\n",
    "            file.write(cryst1_line)\n",
    "            file.writelines(lines)\n",
    "\n",
    "def preprocess_pdb_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pdb'):\n",
    "            pdb_file = os.path.join(directory, filename)\n",
    "            add_cryst1_record(pdb_file)\n",
    "\n",
    "def calculate_chain_clashes(model, chain):\n",
    "    atoms = [atom for atom in model.get_atoms() if atom.get_parent().get_parent() != chain and atom.element != 'H']\n",
    "    ns = NeighborSearch(atoms)\n",
    "    chain_atoms = [atom for atom in chain.get_atoms() if atom.element != 'H']\n",
    "    clashes = 0\n",
    "\n",
    "    for atom in chain_atoms:\n",
    "        neighbors = ns.search(atom.coord, 3.0)\n",
    "        clashes += len([neighbor for neighbor in neighbors if neighbor != atom])\n",
    "    return clashes\n",
    "\n",
    "def count_low_b_factors(chain, threshold):\n",
    "    low_b_factor_count = 0\n",
    "    total_residues = 0\n",
    "    \n",
    "    for residue in chain:\n",
    "        for atom in residue:\n",
    "            if atom.bfactor < threshold:\n",
    "                low_b_factor_count += 1\n",
    "                break\n",
    "        total_residues += 1\n",
    "    \n",
    "    if total_residues == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (low_b_factor_count / total_residues) * 100\n",
    "\n",
    "def calc_rise_and_symmetry(atoms1, atoms2):\n",
    "    coords1 = np.array([atom.get_coord() for atom in atoms1])\n",
    "    coords2 = np.array([atom.get_coord() for atom in atoms2])\n",
    "    \n",
    "    # Superimposición de las coordenadas\n",
    "    super_imposer = Superimposer()\n",
    "    super_imposer.set_atoms(atoms1, atoms2)\n",
    "    super_imposer.apply(atoms1)  # Aplicar la rotación a atoms1\n",
    "\n",
    "    # Calcular los centros de masas una vez alineados\n",
    "    aligned_coords1 = np.array([atom.get_coord() for atom in atoms1])\n",
    "    aligned_coords2 = np.array([atom.get_coord() for atom in atoms2])\n",
    "    centroid1 = np.mean(aligned_coords1, axis=0)\n",
    "    centroid2 = np.mean(aligned_coords2, axis=0)\n",
    "    \n",
    "    # Calcular el rise como la distancia entre los centros de masas alineados\n",
    "    rise = np.linalg.norm(centroid1 - centroid2)\n",
    "    \n",
    "    # Obtener la matriz de rotación y calcular el ángulo de simetría\n",
    "    rot_matrix = super_imposer.rotran[0]\n",
    "    rotation = R.from_matrix(rot_matrix)\n",
    "    symmetry_degrees = rotation.magnitude() * (180 / np.pi)  # Convertir de radianes a grados\n",
    "    \n",
    "    return rise, symmetry_degrees\n",
    "\n",
    "def are_sequences_similar(chain1, chain2, threshold=0.8):\n",
    "    seq1 = ''.join([residue.resname for residue in chain1.get_residues()])\n",
    "    seq2 = ''.join([residue.resname for residue in chain2.get_residues()])\n",
    "\n",
    "    aligner = PairwiseAligner()\n",
    "    alignments = aligner.align(seq1, seq2)\n",
    "    best_alignment = alignments[0]\n",
    "    identity = best_alignment.score / max(len(seq1), len(seq2))\n",
    "    \n",
    "    return identity >= threshold\n",
    "\n",
    "def calculate_symmetry(model):\n",
    "    chain_ids = list(model.child_dict.keys())\n",
    "    rises = []\n",
    "    symmetries = []\n",
    "    symmetry_pairs = []\n",
    "    \n",
    "    for i in range(len(chain_ids)):\n",
    "        for j in range(i + 1, len(chain_ids)):\n",
    "            chain1 = model[chain_ids[i]]\n",
    "            chain2 = model[chain_ids[j]]\n",
    "            \n",
    "            if are_sequences_similar(chain1, chain2):\n",
    "                atoms1 = [atom for atom in chain1.get_atoms() if atom.element != 'H']\n",
    "                atoms2 = [atom for atom in chain2.get_atoms() if atom.element != 'H']\n",
    "                \n",
    "                if len(atoms1) > 0 and len(atoms2) > 0:\n",
    "                    rise, symmetry = calc_rise_and_symmetry(atoms1, atoms2)\n",
    "                    rises.append(rise)\n",
    "                    symmetries.append(symmetry)\n",
    "                    symmetry_pairs.append((chain_ids[i], chain_ids[j], rise, symmetry))\n",
    "    \n",
    "    if len(rises) == 0 or len(symmetries) == 0:\n",
    "        return 0, 0, symmetry_pairs\n",
    "    else:\n",
    "        avg_rise = np.mean(rises)\n",
    "        avg_symmetry = np.mean(symmetries)\n",
    "        return avg_rise, avg_symmetry, symmetry_pairs\n",
    "\n",
    "def count_unstructured_amino_acids_and_clashes(pdb_file, bfactor_threshold):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure('X', pdb_file)\n",
    "        model = structure[0]\n",
    "\n",
    "        dssp = DSSP(model, pdb_file)\n",
    "\n",
    "        unstructured_count = 0\n",
    "        max_unstructured_region = 0\n",
    "        total_clashes = 0\n",
    "        clashes_per_chain = {}\n",
    "        low_b_factors_per_chain = {}\n",
    "\n",
    "        for chain in model:\n",
    "            chain_dssp = [dssp[key] for key in dssp.keys() if key[0] == chain.id]\n",
    "\n",
    "            if not chain_dssp:\n",
    "                continue\n",
    "\n",
    "            ss = [aa[2] for aa in chain_dssp]\n",
    "            ss_string = ''.join(ss)\n",
    "            #print(f\"Chain {chain.id} DSSP data: {ss_string}\")\n",
    "\n",
    "            first_structured = next((i for i, s in enumerate(ss) if s != '-'), None)\n",
    "            last_structured = next((i, s) for i, s in enumerate(reversed(ss)) if s != '-')\n",
    "            if last_structured is not None:\n",
    "                last_structured = len(ss) - 1 - last_structured[0]\n",
    "\n",
    "            if first_structured is None or last_structured is None:\n",
    "                continue\n",
    "\n",
    "            current_unstructured_count = 0\n",
    "            for s in ss[first_structured:last_structured+1]:\n",
    "                if s == '-':\n",
    "                    current_unstructured_count += 1\n",
    "                else:\n",
    "                    if current_unstructured_count > max_unstructured_region:\n",
    "                        max_unstructured_region = current_unstructured_count\n",
    "                    current_unstructured_count = 0\n",
    "            if current_unstructured_count > max_unstructured_region:\n",
    "                max_unstructured_region = current_unstructured_count\n",
    "\n",
    "            unstructured_count += sum(1 for s in ss[first_structured:last_structured+1] if s == '-')\n",
    "\n",
    "            clashes = calculate_chain_clashes(model, chain)\n",
    "            total_clashes += clashes\n",
    "            clashes_per_chain[chain.id] = clashes\n",
    "\n",
    "            low_b_factors = count_low_b_factors(chain, bfactor_threshold)\n",
    "            low_b_factors_per_chain[chain.id] = low_b_factors\n",
    "\n",
    "        avg_rise, avg_symmetry, symmetry_pairs = calculate_symmetry(model)\n",
    "\n",
    "        return unstructured_count, max_unstructured_region, total_clashes, clashes_per_chain, low_b_factors_per_chain, avg_rise, avg_symmetry, symmetry_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdb_file}: {e}\")\n",
    "        return None, None, None, None, None, None, None, None\n",
    "\n",
    "def process_pdb_file(args):\n",
    "    pdb_file, bfactor_threshold = args\n",
    "    results = count_unstructured_amino_acids_and_clashes(pdb_file, bfactor_threshold)\n",
    "    if results[0] is not None:\n",
    "        unstructured_count, max_unstructured_region, total_clashes, clashes_per_chain, low_b_factors_per_chain, avg_rise, avg_symmetry, symmetry_pairs = results\n",
    "        row = {\n",
    "            'Name': os.path.basename(pdb_file),\n",
    "            'Unstructured_count': unstructured_count,\n",
    "            'Max_unstructured_region': max_unstructured_region,\n",
    "            'Total_clashes': total_clashes,\n",
    "            'Average_rise': avg_rise,\n",
    "            'Average_symmetry': avg_symmetry\n",
    "        }\n",
    "        for chain_id, clashes in clashes_per_chain.items():\n",
    "            row[f'Clashes_chain_{chain_id}'] = clashes\n",
    "        for chain_id, low_b_factors in low_b_factors_per_chain.items():\n",
    "            row[f'Low_B_factors_chain_{chain_id}'] = low_b_factors\n",
    "        \n",
    "        for chain1, chain2, rise, symmetry in symmetry_pairs:\n",
    "            row[f'Rise_{chain1}_{chain2}'] = rise\n",
    "            row[f'Symmetry_{chain1}_{chain2}'] = symmetry\n",
    "        \n",
    "        return row\n",
    "    return None\n",
    "\n",
    "def main(path, directories, bfactor_threshold, num_threads):\n",
    "    #patron  = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.pdb'\n",
    "    patron = r'(.*(\\d+)\\.pdb$)'\n",
    "    for directory in directories:\n",
    "        directory = os.path.join(path, directory)\n",
    "        preprocess_pdb_files(directory)\n",
    "\n",
    "    pdb_files = []\n",
    "    for directory in directories:\n",
    "        directory = os.path.join(path, directory)\n",
    "        for filename in os.listdir(directory):\n",
    "            if re.match(patron, filename):\n",
    "                pdb_file = os.path.join(directory, filename)\n",
    "                pdb_files.append((pdb_file, bfactor_threshold))\n",
    "\n",
    "    with Pool(num_threads) as pool:\n",
    "        data = pool.map(process_pdb_file, pdb_files)\n",
    "\n",
    "    data = [row for row in data if row is not None]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# def main(path, directories, bfactor_threshold):\n",
    "#     patron = r'(.*(\\d+)\\.pdb$)'\n",
    "#     for directory in directories:\n",
    "#         directory = os.path.join(path, directory)\n",
    "#         preprocess_pdb_files(directory)\n",
    "#     data = []\n",
    "\n",
    "#     for directory in directories:\n",
    "#         directory = os.path.join(path, directory)\n",
    "#         for filename in os.listdir(directory):\n",
    "#             if re.match(patron, filename):\n",
    "#                 pdb_file = os.path.join(directory, filename)\n",
    "#                 results = count_unstructured_amino_acids_and_clashes(pdb_file, bfactor_threshold)\n",
    "#                 if results[0] is not None:\n",
    "#                     unstructured_count, max_unstructured_region, total_clashes, clashes_per_chain, low_b_factors_per_chain, avg_rise, avg_symmetry, symmetry_pairs = results\n",
    "#                     row = {\n",
    "#                         'Name': filename,\n",
    "#                         'Unstructured_count': unstructured_count,\n",
    "#                         'Max_unstructured_region': max_unstructured_region,\n",
    "#                         'Total_clashes': total_clashes,\n",
    "#                         #'Average_rise': avg_rise,\n",
    "#                         #'Average_symmetry': avg_symmetry\n",
    "#                     }\n",
    "#                     for chain_id, clashes in clashes_per_chain.items():\n",
    "#                         row[f'Clashes_chain_{chain_id}'] = clashes\n",
    "#                     for chain_id, low_b_factors in low_b_factors_per_chain.items():\n",
    "#                         row[f'Low_B_factors_chain_{chain_id}'] = low_b_factors\n",
    "                    \n",
    "#                     # Añadir las simetrías por par de cadenas al DataFrame\n",
    "#                     for chain1, chain2, rise, symmetry in symmetry_pairs:\n",
    "#                         row[f'Rise_{chain1}_{chain2}'] = rise\n",
    "#                         row[f'Symmetry_{chain1}_{chain2}'] = symmetry\n",
    "                    \n",
    "#                     data.append(row)\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "#     return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "\n",
    "bfactor_threshold = 50  # Umbral de B-factor\n",
    "num_threads = 20\n",
    "df_loop_clashes = main(directorio,carpetas,bfactor_threshold, num_threads)\n",
    "\n",
    "# Imprime el dataframe resultante\n",
    "print(df_loop_clashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1884da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join df_loop_clashes and df_pydock by Name\n",
    "df_pydock = df_pydock.merge(df_loop_clashes, on= 'Name', how='left')\n",
    "df_pydock\n",
    "#T254 No Knots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de5feb",
   "metadata": {},
   "source": [
    "*Detección de nudos*\n",
    "\n",
    "Si existe probelmas, quitar el check nots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## si hay error comentar esta funcion , si esta mas alla de 40 minutos  descomentar la siguiente##\n",
    "#def check_knots_and_get_info(pdb_file):\n",
    " #    command = [\"knot_pull_check\", \"-kq\", pdb_file]                                              \n",
    "  #   try:\n",
    "   #      result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    #     output = result.stdout.strip()\n",
    "     #   \n",
    "      #   # Analiza la salida para determinar si contiene '#'\n",
    "       #  if '#' in output:\n",
    "        #     return (pdb_file, 'yes')\n",
    "         #else:\n",
    "          #   return (pdb_file, 'no')\n",
    "    # except subprocess.CalledProcessError as e:\n",
    "        # print(f\"Error executing command: {e}\")                                                 \n",
    "    #return (pdb_file, 'no')  # Asumimos 'no' si hay un error al ejecutar el comando\n",
    "\n",
    "\n",
    "## descomentar si es necesario##\n",
    "def check_knots_and_get_info(pdb_file):\n",
    "                                                    \n",
    "     return (pdb_file, 'no')  # Asumimos 'no' si hay un error al ejecutar el comando\n",
    "\n",
    "def process_pdb_files(path, directories, num_workers):\n",
    "    patron = r'(.*(\\d+)\\.pdb$)'\n",
    "   # patron = r'fold_t\\d+_b[a-z]+_a_\\d+_model_\\d+_supeimp\\.pdb'\n",
    "    data = []\n",
    "\n",
    "    pdb_files = []\n",
    "    for directory in directories:\n",
    "        directory_path = os.path.join(path, directory)\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if re.match(patron, filename):\n",
    "                pdb_file = os.path.join(directory_path, filename)\n",
    "                pdb_files.append(pdb_file)\n",
    "    \n",
    "    # Usar ProcessPoolExecutor para paralelizar la ejecución con un número específico de trabajadores\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_pdb = {executor.submit(check_knots_and_get_info, pdb_file): pdb_file for pdb_file in pdb_files}\n",
    "        for future in as_completed(future_to_pdb):\n",
    "            pdb_file = future_to_pdb[future]\n",
    "            try:\n",
    "                filename, knot_info = future.result()\n",
    "                row = {\n",
    "                    'Name': os.path.basename(filename),\n",
    "                    'Knots': knot_info\n",
    "                }\n",
    "                data.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {pdb_file}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "num_workers = 20  # Número de hilos a utilizar\n",
    "df_knots = process_pdb_files(directorio,carpetas,num_workers)\n",
    "\n",
    "df_knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c259431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join df_knots and df_pydock by Name\n",
    "df_pydock = df_pydock.merge(df_knots, on= 'Name', how='left')\n",
    "df_pydock.to_csv(directorio_csv+'/pydock4_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a72566e0",
   "metadata": {},
   "source": [
    "#### 3.3. Log.txt information retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock\n",
    "# Folders of all models\n",
    "carpetas_log = [nombre for nombre in os.listdir(directorio) if os.path.isdir(os.path.join(directorio, nombre))]\n",
    "#carpetas_log.remove('Version1')\n",
    "carpetas_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe columns\n",
    "columns = [\"Complex\",\"Model\",\"State\",'Version', 'Recycle', 'pLDDT', 'pTM', 'ipTM', 'tol','Seed']\n",
    "\n",
    "# Patrons in the text to gather the information\n",
    "#complex_pattern = re.compile(r'((T|t)\\/.*_A)') \n",
    "rank_pattern = re.compile(r'(rank_(\\d+))|(pred_\\d+)|(ranked_.*)')\n",
    "model_pattern = re.compile(r'model_(\\d+)')\n",
    "state_pattern = re.compile(r'rank')\n",
    "version_pattern = re.compile(r\"((deepfold|alphafold2_multimer)_v\\d+)_model\")\n",
    "recycle_pattern = re.compile(r'recycle=(\\d+)')\n",
    "plddt_pattern = re.compile(r'pLDDT=([\\d.]+)')\n",
    "ptm_pattern = re.compile(r'pTM=([\\d.]+)')\n",
    "iptm_pattern = re.compile(r'ipTM=([\\d.]+)')\n",
    "tol_pattern = re.compile(r'tol=([\\d.]+)')\n",
    "seed_pattern = re.compile(r'seed_([\\d.]+)')\n",
    "name_pattern = re.compile(r\"(fold_t\\d+_\\d+_model_\\d+)\")\n",
    "df_log=pd.DataFrame()\n",
    "\n",
    "for carpeta in carpetas_log:\n",
    "    directorio_log=f\"{directorio}/{carpeta}/log.txt\"\n",
    "    # Loading the archive\n",
    "    with open(directorio_log, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Value extraction\n",
    "    #name = None\n",
    "    complex=None\n",
    "    model=None\n",
    "    version = None\n",
    "    state=None\n",
    "    recycle = None\n",
    "    plddt = None\n",
    "    ptm = None\n",
    "    iptm = None\n",
    "    tol = None\n",
    "    seed = None\n",
    "    data=[]\n",
    "    for line in lines:\n",
    "        \n",
    "        # # Complex\n",
    "        # match = complex_pattern.search(line)\n",
    "        # if match:\n",
    "        #     print()\n",
    "        #     complex = match.group(0)\n",
    "        #     complex=complex[2:-2]\n",
    "\n",
    "        #Name\n",
    "        # match = name_pattern.search(line)\n",
    "        # if match:\n",
    "        #     name = match.group(1)+'.pdb'\n",
    "        # else:\n",
    "        #     name =directorio_log \n",
    "        #State\n",
    "        match = state_pattern.search(line)\n",
    "        if match:\n",
    "            state=\"relaxed\"\n",
    "        else:\n",
    "            state=\"unrelaxed\"\n",
    "\n",
    "        # Model\n",
    "        match = model_pattern.search(line)\n",
    "        if match:\n",
    "            model= match.group(1)\n",
    "        \n",
    "        # Version\n",
    "        match = version_pattern.search(line)\n",
    "        if match:\n",
    "            version = match.group(1)\n",
    "        else:\n",
    "            version = 'alphafold3'\n",
    "            \n",
    "        # Recycle\n",
    "        match = recycle_pattern.search(line)\n",
    "        if match:\n",
    "            recycle = match.group(1)\n",
    "        else:\n",
    "            recycle = 'Seed_0'\n",
    "        \n",
    "        #  pLDDT\n",
    "        match = plddt_pattern.search(line)\n",
    "        if match:\n",
    "            plddt = match.group(1)\n",
    "        else:\n",
    "            plddt = None\n",
    "        \n",
    "        #  pTM\n",
    "        match = ptm_pattern.search(line)\n",
    "        if match:\n",
    "            ptm = match.group(1)\n",
    "        else:\n",
    "            ptm=None\n",
    "        \n",
    "        #  ipTM\n",
    "        match = iptm_pattern.search(line)\n",
    "        if match:\n",
    "            iptm = match.group(1)\n",
    "        else:\n",
    "            iptm=None\n",
    "        \n",
    "        #  tol\n",
    "        match = tol_pattern.search(line)\n",
    "        if match:\n",
    "            tol = match.group(1)\n",
    "        else:\n",
    "            tol=\"-\"\n",
    "        \n",
    "        #seed\n",
    "        match = seed_pattern.search(line)\n",
    "        if match:\n",
    "            seed = match.group(1)\n",
    "        else:\n",
    "            seed=\"-\"\n",
    "        \n",
    "        # rank\n",
    "        match = rank_pattern.search(line)\n",
    "        if match:   \n",
    "            recycle = 'Seed_0'\n",
    "        # Guardar los valores en el DataFrame\n",
    "        data.append([complex,model,state,version, recycle, plddt, ptm, iptm, tol,seed])\n",
    "\n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Convierte las columnas 'ipTM' y 'pTM' a tipos de datos numéricos (flotantes)\n",
    "    df['ipTM'] = pd.to_numeric(df['ipTM'], errors='coerce')\n",
    "    df['pTM'] = pd.to_numeric(df['pTM'], errors='coerce')\n",
    "    \n",
    "    \n",
    "    # Addicion de Model confidence según la formula del articulo\n",
    "    df['Model_confidence'] = 0.8 * df['ipTM'] + 0.2 * df['pTM']\n",
    "    if carpeta.startswith('fold'):\n",
    "        df[\"Complex\"]=carpeta.split('_')[1].upper()\n",
    "    else:\n",
    "        df[\"Complex\"]=carpeta[0:4]\n",
    "    df = df.dropna(subset=['Model_confidence'])\n",
    "    df['pLDDT'] = pd.to_numeric(df['pLDDT'], errors='coerce')\n",
    "    df_log=pd.concat([df_log,df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aef76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The models were all relaxed\n",
    "df_log[\"State\"]=\"relaxed\"\n",
    "df_log.to_csv(directorio_csv+'/log_all.csv', index=False)\n",
    "df_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24f4d664",
   "metadata": {},
   "source": [
    "### 4.  Final fusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44be3403",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;\">Now we ensemble a new_dataframe to collect all the data obtained during the calculation of for a posterior statistical analysis\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ffa3f3a",
   "metadata": {},
   "source": [
    "#### 4.1 Checking for possible issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d11e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataframes\n",
    "df_pydock =pd.read_csv(directorio_csv+'/pydock4_all.csv')\n",
    "df_log=pd.read_csv(directorio_csv+'/log_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22307bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pydock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e77c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking for the length of dataframes, be aware of the existence of the entries from cristals!\n",
    "unicos=set(df_pydock[\"Complex\"])\n",
    "for complejo in unicos:\n",
    "    print(complejo)\n",
    "    mu=len(df_pydock[df_pydock[\"Complex\"]==complejo])\n",
    "    #nu=len(df_rmsd[df_rmsd[\"Complex\"]==complejo])\n",
    "    clus=len(df_log[df_log[\"Complex\"]==complejo])\n",
    "    #print( datos_carpeta[complejo])\n",
    "    print(\"Pydock:\",mu,\"RMSD:\", \" Log:\",clus,\"Diferencia:\",mu,mu-clus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb6c7f24",
   "metadata": {},
   "source": [
    "#### 4.2 Merging dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29c9bf0e",
   "metadata": {},
   "source": [
    "Determaining which columns are diferent and merging by the common ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columna4=(df_pydock.columns).tolist()\n",
    "columna3=(df_log.columns).tolist()\n",
    "compartidos2=list(set(columna4).intersection(columna3))\n",
    "#compartidos2=['Name']\n",
    "df_pydock[compartidos2]=df_pydock[compartidos2].astype(str)\n",
    "df_log[compartidos2]=df_log[compartidos2].astype(str)\n",
    "merged_df2 = df_pydock.merge(df_log, on= compartidos2, how='left')\n",
    "print (compartidos2)\n",
    "merged_df2.to_csv(directorio_csv+'/merged_df2.csv')\n",
    "#merged_df2[\"Total_Name\"]=merged_df2[\"Complex\"]+\"_\"+merged_df2[\"Name\"]\n",
    "\n",
    "#  Lo siguiente es para poner los Seed_0 iguales que los del 20, activar y corregir si es necesario de normal quitamos los seed_0\n",
    "#mask = merged_df2[\"Recycle\"] == \"Seed_0\"\n",
    "#merged_df2.loc[merged_df2[\"Recycle\"] == \"20\", [\"pLDDT\", \"pTM\", \"ipTM\", \"tol\"]]\n",
    "#merged_df2.loc[mask, [\"pLDDT\", \"pTM\", \"ipTM\", \"tol\"]] = merged_df2.values[:len(merged_df2[mask])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bee56",
   "metadata": {},
   "source": [
    "### 4.3 Filter the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pydock_advanced(\n",
    "    df, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "    symmetry_conditions, res_conditions, invert=False\n",
    "):\n",
    "    # Filtro inicial basado en Knots_value, Max_unstructured_region y Total_clashes\n",
    "    main_filter = (\n",
    "        (df['Knots'] == Knots_value) &\n",
    "        (df['Max_unstructured_region'] <= Max_unstructured_region) &\n",
    "        (df['Total_clashes'] <= Total_clashes)\n",
    "    )\n",
    "\n",
    "    # Construye las condiciones de simetría dinámicamente\n",
    "    symmetry_filter = None\n",
    "    for condition in symmetry_conditions:\n",
    "        Symmetry_col, up, low = condition\n",
    "        if Symmetry_col not in df.columns:\n",
    "            warnings.warn(f\"Column '{Symmetry_col}' does not exist in the DataFrame.\")\n",
    "            continue\n",
    "        current_filter = (\n",
    "            (df[Symmetry_col].between(-up, -low)) | \n",
    "            (df[Symmetry_col].between(low, up))\n",
    "        )\n",
    "        if symmetry_filter is None:\n",
    "            symmetry_filter = current_filter\n",
    "        else:\n",
    "            symmetry_filter |= current_filter\n",
    "    \n",
    "    if symmetry_filter is not None:\n",
    "        main_filter &= symmetry_filter\n",
    "\n",
    "    # Construye las condiciones de Res_with_low_pLDDT dinámicamente\n",
    "    res_filter = None\n",
    "    for condition in res_conditions:\n",
    "        Res_col, threshold = condition\n",
    "        if Res_col not in df.columns:\n",
    "            warnings.warn(f\"Column '{Res_col}' does not exist in the DataFrame.\")\n",
    "            continue\n",
    "        # Verificar si el valor en df[Res_col] es 0, en cuyo caso se ignora el filtro\n",
    "        if df[Res_col].eq(0).all():\n",
    "            continue\n",
    "        current_filter = (df[Res_col] <= threshold)\n",
    "        if res_filter is None:\n",
    "            res_filter = current_filter\n",
    "        else:\n",
    "            res_filter &= current_filter\n",
    "    \n",
    "    if res_filter is not None:\n",
    "        main_filter &= res_filter\n",
    "\n",
    "    # Aplicar el filtro inverso si invert es True\n",
    "    if invert:\n",
    "        filtered_df = df[~main_filter]\n",
    "    else:\n",
    "        filtered_df = df[main_filter]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b9252",
   "metadata": {},
   "source": [
    "Aplicacion de la funcion\n",
    "\n",
    "- Max_unstructured_region\n",
    "- Total_clashes\n",
    "- res_conditions: [('Low_B_factors_chain_A', 20)] se cambia el 20 segun el bfactor deseado para filtrar ( % de residuos de la secuencia con un valor de ppldt <50)\n",
    "- Knots_value: yes or 'no' \n",
    "- (opcional) symmetry_conditions: [('Symmetry_A_F', 185, 175)] intervalo de simetria deseado para filtrar. Para cadena repetidas que forman estructuras simetricas, no tiene porque ser simetrico, segun la estructura ( ver a ojo lo modelado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 4, 55, \\\n",
    "#     [('Low_B_factors_chain_A', 20), ('Low_B_factors_chain_B', 20)], \\\n",
    "#     'no', [('Symmetry_A_B', 185, 175)] # T242 incluimos la simtria  180-+5  E_F\n",
    "#Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 8, 55, \\\n",
    "#     [('Low_B_factors_chain_A', 7), ('Low_B_factors_chain_B', 7)], \\\n",
    "#     'no', [('Symmetry_A_B', 185, 175)]# T244 incluimos, no se calcula Symmetry \n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 55, \\\n",
    "    # [('Low_B_factors_chain_A', 9), ('Low_B_factors_chain_B', 12)], \\\n",
    "    # 'no', [('Symmetry_A_B', 185, 175)]# T248 incluimos, no se calcula Symmetry \n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 500, \\\n",
    "#     [('Low_B_factors_chain_A', 70), ('Low_B_factors_chain_B', 70), ('Low_B_factors_chain_C', 70)], \\\n",
    "#     'no', [('Symmetry_B_C', 125, 115),('Symmetry_A_B', 125, 115)]# T250 T252 incluimos, se calcula Symmetry \n",
    "#Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 300, [('Low_B_factors_chain_A', 10), ('Low_B_factors_chain_B', 10), ('Low_B_factors_chain_C', 10),('Low_B_factors_chain_D', 10),('Low_B_factors_chain_E', 10)], 'no', [('Symmetry_B_C', 125, 115),('Symmetry_A_B', 125, 115)]# T256 \n",
    "\n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 6, 500, \\\n",
    "#     [('Low_B_factors_chain_A', 10), ('Low_B_factors_chain_B', 10), ('Low_B_factors_chain_C', 10),('Low_B_factors_chain_D', 10)], \\\n",
    "#     'no', [('Symmetry_A_B', 185, 175)]# T254 T255 incluimos, se calcula Symmetry \n",
    "\n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 20, 300, \\\n",
    "#     [('Low_B_factors_chain_A', 66), ('Low_B_factors_chain_B', 66), ('Low_B_factors_chain_C', 66),('Low_B_factors_chain_D', 66),('Low_B_factors_chain_E', 66)], \\\n",
    "#     'no', [('Symmetry_A_F', 185, 175)]# T262, se uss Symmetry_A_F que no exite para no calcular filtro por Symmetria \n",
    "\n",
    "\n",
    "### T264, se usa Symmetry_A_Z que no exite para no calcular filtro por Symmetria ##\n",
    "# Max_unstructured_region, Total_clashes, res_conditions, Knots_value, symmetry_conditions = 24, 1000, \\\n",
    "#     [('Low_B_factors_chain_A', 24),('Low_B_factors_chain_B', 24),('Low_B_factors_chain_C', 24),('Low_B_factors_chain_D', 20)], \\\n",
    "#     'no', [('Symmetry_A_Z', 185, 175)]\n",
    "\n",
    "\n",
    "### T266, se usa Symmetry_A_F que no exite para no calcular filtro por Symmetria ##\n",
    "# Max_unstructured_region, Total_clashes,\\\n",
    "# res_conditions,\\\n",
    "# Knots_value, symmetry_conditions =\\\n",
    "#     20, 60, \\\n",
    "#     [('Low_B_factors_chain_A', 20)], \\\n",
    "#     'no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "\n",
    "\n",
    "## Customizable\n",
    "# Max_unstructured_region, Total_clashes=20, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 20)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "# Max_unstructured_region, Total_clashes=20, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 20)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "#T168\n",
    "# Max_unstructured_region, Total_clashes=7, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 80)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "# filtered_df = filter_pydock_advanced(\n",
    "#     merged_df2, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "#     symmetry_conditions=symmetry_conditions,\n",
    "#     res_conditions=res_conditions\n",
    "#)\n",
    "#T170\n",
    "# Max_unstructured_region, Total_clashes=7, 60\n",
    "# res_conditions=[('Low_B_factors_chain_A', 80)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_F', 185, 175)]\n",
    "\n",
    "# #T172\n",
    "# merged_df2=pd.read_csv(directorio_csv+'/pydock4_all.csv')\n",
    "# Max_unstructured_region, Total_clashes=12, 5000\n",
    "# res_conditions=[('Clashes_chain_A', 600)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_B_D', 185, 175),()]\n",
    "\n",
    "# #T280\n",
    "# merged_df2=pd.read_csv(directorio_csv+'/merged_df2.csv')\n",
    "# Max_unstructured_region, Total_clashes=8, 150\n",
    "# res_conditions=[('Low_B_factors_chain_A', 8),('Low_B_factors_chain_B', 8),('Low_B_factors_chain_C', 8),('Low_B_factors_chain_D', 8)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 185, 175)]\n",
    "\n",
    "#T288\n",
    "# Max_unstructured_region, Total_clashes=6, 110,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 21),('Low_B_factors_chain_B', 21),(\"Low_B_factors_chain_C\",21)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 140, 100),(\"Symmetry_B_C\",140, 100)]\n",
    "\n",
    "#T290\n",
    "# Max_unstructured_region, Total_clashes=2, 97,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 5),('Low_B_factors_chain_B', 5),(\"Low_B_factors_chain_C\",5),(\"Low_B_factors_chain_D\",5),(\"Low_B_factors_chain_E\",5),(\"Low_B_factors_chain_F\",5)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 65, 55),(\"Symmetry_B_C\",65, 55)]\n",
    "\n",
    "# #T290\n",
    "# Max_unstructured_region, Total_clashes=4, 50,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 5),('Low_B_factors_chain_B', 50)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 65, 55)]\n",
    "\n",
    "# #T292\n",
    "# Max_unstructured_region, Total_clashes=7, 400,\n",
    "# res_conditions=[('Low_B_factors_chain_A', 14),('Low_B_factors_chain_B', 14),(\"Low_B_factors_chain_C\",14),(\"Low_B_factors_chain_D\",14),(\"Low_B_factors_chain_E\",14),(\"Low_B_factors_chain_F\",14),(\"Low_B_factors_chain_G\",14),(\"Low_B_factors_chain_H\",14),(\"Low_B_factors_chain_I\",14)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 135, 105),(\"Symmetry_B_C\",135, 105)]\n",
    "\n",
    "# # T282\n",
    "# Max_unstructured_region, Total_clashes=6, 190,\n",
    "# bfactor_1=3\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1),(\"Low_B_factors_chain_D\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_A_C', 190, 170),(\"Symmetry_B_D\",190, 170)]\n",
    "\n",
    "# T286\n",
    "# Max_unstructured_region, Total_clashes=8, 400,\n",
    "# bfactor_1=7\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1),(\"Low_B_factors_chain_D\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_B_C\",140, 100)]\n",
    "\n",
    "# T296\n",
    "# Max_unstructured_region, Total_clashes=5, 32,\n",
    "# #bfactor_1=7\n",
    "# res_conditions=[('Low_B_factors_chain_A', 5),('Low_B_factors_chain_B', 100)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_B_C\",140, 100)]\n",
    "\n",
    "# T294\n",
    "# Max_unstructured_region, Total_clashes=5, 130,\n",
    "# #bfactor_1=7\n",
    "# res_conditions=[('Low_B_factors_chain_A', 2),('Low_B_factors_chain_B', 2),(\"Low_B_factors_chain_C\",7),(\"Low_B_factors_chain_D\",7), (\"Low_B_factors_chain_E\",17),(\"Low_B_factors_chain_F\",17)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",190, 160), (\"Symmetry_C_D\",190, 160), (\"Symmetry_E_F\",50, 0)]\n",
    "# T298 \n",
    "# Max_unstructured_region, Total_clashes=6, 90,\n",
    "# bfactor_1=2.5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",125, 115), (\"Symmetry_B_C\",125, 115)]\n",
    "# T300\n",
    "# Max_unstructured_region, Total_clashes=6, 120,\n",
    "# bfactor_1=5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1),('Low_B_factors_chain_B', bfactor_1),(\"Low_B_factors_chain_C\",bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",185, 175), (\"Symmetry_C_D\",185, 175)]\n",
    "# T304 \n",
    "# Max_unstructured_region, Total_clashes=6, 16,\n",
    "# bfactor_1=5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_H_B\",185, 175)] \n",
    "# T302 \n",
    "# Max_unstructured_region, Total_clashes=4, 280,\n",
    "# bfactor_1=5\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",125, 115)] \n",
    "# T306\n",
    "# Max_unstructured_region, Total_clashes=3, 10,\n",
    "# bfactor_1=1.6\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [(\"Symmetry_A_B\",125, 115)] \n",
    "# T308\n",
    "# Max_unstructured_region, Total_clashes=4, 200,\n",
    "# bfactor_1=9.4\n",
    "# res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "# Knots_value, symmetry_conditions ='no', [('Symmetry_C_D', 185, 175)] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500521e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Parando la ejecución aquí.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2=pd.read_csv(directorio_csv+'/merged_df2.csv')\n",
    "merged_df2.drop_duplicates(inplace=True,subset=\"Name\")\n",
    "merged_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a321b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T309\n",
    "Max_unstructured_region, Total_clashes=5, 30,\n",
    "bfactor_1=1\n",
    "res_conditions=[('Low_B_factors_chain_A', bfactor_1)]\n",
    "Knots_value, symmetry_conditions ='no', [('Symmetry_A_B', 185, 175)] \n",
    "\n",
    "\n",
    "filtered_df = filter_pydock_advanced(\n",
    "    merged_df2, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "    symmetry_conditions=symmetry_conditions,\n",
    "    res_conditions=res_conditions\n",
    ")\n",
    "\n",
    "result_df_inverted = filter_pydock_advanced(\n",
    "    merged_df2, Knots_value, Max_unstructured_region, Total_clashes,\n",
    "    symmetry_conditions=symmetry_conditions,\n",
    "    res_conditions=res_conditions,\n",
    "    invert=True\n",
    ")\n",
    "print(len(filtered_df))\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62402950",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(directorio_csv+'/pydock4_all_filtered.csv', index=False)\n",
    "result_df_inverted.to_csv(directorio_csv+'/pydock4_all_filtered_inv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Max_unstructured_region,Total_clashes,Res_with_low_pLDDT_A,Res_with_low_pLDDT_B,Knots_value = [10,90,20,20,'no']# T236\n",
    "# #Max_unstructured_region,Total_clashes,Res_with_low_pLDDT_A,Res_with_low_pLDDT_B,Knots_value = [5,70,15,20,'no']# T238\n",
    "# Max_unstructured_region,Total_clashes,Res_with_low_pLDDT_E,Res_with_low_pLDDT_F,Knots_value = [5,800,20,20,'no']# T240\n",
    "\n",
    "# filtered_df = merged_df2[\n",
    "#     (merged_df2['Knots'] == 'no') &\n",
    "#     (merged_df2['Max_unstructured_region'] <= Max_unstructured_region) &\n",
    "#     (merged_df2['Total_clashes'] <= Total_clashes) &\n",
    "#     #(merged_df2['Res_with_low_pLDDT_A'] <= Res_with_low_pLDDT_A) & #T236 T238\n",
    "#     #(merged_df2['Res_with_low_pLDDT_B'] <= Res_with_low_pLDDT_B)   #T236 T238\n",
    "#     (merged_df2['Res_with_low_pLDDT_E'] <= Res_with_low_pLDDT_E) &  #T240\n",
    "#     (merged_df2['Res_with_low_pLDDT_F'] <= Res_with_low_pLDDT_F)    #T240\n",
    "# ]\n",
    "\n",
    "# result_df_inverted = merged_df2[\n",
    "#     ~(\n",
    "#         (merged_df2['Knots'] == Knots_value) &\n",
    "#         (merged_df2['Max_unstructured_region'] <= Max_unstructured_region) &\n",
    "#         (merged_df2['Total_clashes'] <= Total_clashes) &\n",
    "#         #(merged_df2['Res_with_low_pLDDT_A'] <= Res_with_low_pLDDT_A) & #T236 T238\n",
    "#         #(merged_df2['Res_with_low_pLDDT_B'] <= Res_with_low_pLDDT_B)   #T236 T238\n",
    "#         (merged_df2['Res_with_low_pLDDT_E'] <= Res_with_low_pLDDT_E) &  #T240\n",
    "#         (merged_df2['Res_with_low_pLDDT_F'] <= Res_with_low_pLDDT_F)    #T240\n",
    "#     )\n",
    "# ]\n",
    "# # Resultado final\n",
    "# filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df_inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c76b3",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ca1f0",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27adfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm=pd.read_csv(directorio_csv+'/pydock4_all_filtered.csv')\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm=pd.read_csv(directorio_csv+'/pydock4_all_filtered.csv')\n",
    "\n",
    "# Removing unnecesary columns\n",
    "columnas=['Conf','RANK']\n",
    "df_norm.drop(columnas, axis=1, inplace=True)\n",
    "df_norm.dropna(subset=[\"Complex\"],inplace=True)\n",
    "\n",
    "# Removing duplicates\n",
    "df_norm=df_norm.drop_duplicates(subset=[\"Name\"],keep=\"first\")\n",
    "duplicados = df_norm[df_norm.duplicated(subset=[\"Name\",\"Version\",\"Complex\",\"Recycle\",\"State\"])]\n",
    "\n",
    "# Adding Total2 column\n",
    "df_norm[\"Total2\"]=df_norm[\"VDW\"]+df_norm[\"Ele\"]+df_norm[\"Desolv\"] \n",
    "\n",
    "# Z-Score individuales, inicializacion\n",
    "df_norm[\"MCZ-Score\"] = 0 # Z-score de model_conficence\n",
    "df_norm[\"PLDDTZ-Score\"] = 0 # Z-score de pLDDT\n",
    "df_norm[\"TEZ-Score\"] = 0 # Z-score de Total\n",
    "df_norm[\"TE2Z-Score\"] = 0 # Z-score de Total2\n",
    "\n",
    "# Suma de Z-Score, inicialicion\n",
    "df_norm[\"Sum_Z\"] = 0 # Z-score Model confidence + Total\n",
    "df_norm[\"Sum2_Z\"] = 0 # Z-score Model confidence + Total2\n",
    "df_norm[\"Z-PLT\"] = 0 # Z-score de pLDDT + Total\n",
    "df_norm[\"Z-PLT2\"]= 0 # Z-score de pLDDT + Total2\n",
    "\n",
    "# Ranking Z-Score, inicializacion\n",
    "df_norm[\"Ranking_Z\"] = 0 # Ranking de Sum_Z\n",
    "df_norm[\"Ranking2_Z\"] = 0 # Ranking de Sum2_Z\n",
    "df_norm[\"Ranking_PLT\"] = 0 # Ranking de Z-PLT\n",
    "df_norm[\"Ranking_PLT2\"] = 0 # Ranking de Z-PLT2\n",
    "\n",
    "# Calculo de medias y desviaciones segun complejo\n",
    "grouped = df_norm.groupby([\"Complex\"])\n",
    "medias=grouped.mean()\n",
    "sdesv=grouped.std()\n",
    "\n",
    "# Z-Score individuales\n",
    "for name, group in grouped:\n",
    "    # Calculamos Z_score de model_conficence y total energy\n",
    "    df_norm.loc[group.index,[\"MCZ-Score\"]] = (group[\"Model_confidence\"]-medias.loc[name,\"Model_confidence\"])/sdesv.loc[name,\"Model_confidence\"]\n",
    "    df_norm.loc[group.index,[\"TEZ-Score\"]] = (group[\"Total\"]-medias.loc[name,\"Total\"])/sdesv.loc[name,\"Total\"]\n",
    "    df_norm.loc[group.index,[\"TE2Z-Score\"]] = (group[\"Total2\"]-medias.loc[name,\"Total2\"])/sdesv.loc[name,\"Total2\"]\n",
    "    df_norm.loc[group.index,[\"PLDDTZ-Score\"]] = (group[\"pLDDT\"]-medias.loc[name,\"pLDDT\"])/sdesv.loc[name,\"pLDDT\"]\n",
    "\n",
    "# Suma de Z-Score\n",
    "df_norm.loc[:,\"Sum_Z\"]=df_norm.loc[:,\"MCZ-Score\"]-df_norm.loc[:,\"TEZ-Score\"]\n",
    "df_norm.loc[:,\"Sum2_Z\"]=df_norm.loc[:,\"MCZ-Score\"]-df_norm.loc[:,\"TE2Z-Score\"]\n",
    "df_norm.loc[:,\"Z-PLT\"]=df_norm.loc[:,\"PLDDTZ-Score\"]-df_norm.loc[:,\"TEZ-Score\"]\n",
    "df_norm.loc[:,\"Z-PLT2\"]=df_norm.loc[:,\"PLDDTZ-Score\"]-df_norm.loc[:,\"TE2Z-Score\"]\n",
    "\n",
    "# Ranking Z-Score\n",
    "for name, group in grouped:\n",
    "    df_norm.loc[group.index,\"Ranking_Z\"]=df_norm.loc[group.index,\"Sum_Z\"].rank(ascending=False)\n",
    "    df_norm.loc[group.index,\"Ranking2_Z\"]=df_norm.loc[group.index,\"Sum2_Z\"].rank(ascending=False)\n",
    "    df_norm.loc[group.index,\"Ranking_PLT\"]=df_norm.loc[group.index,\"Z-PLT\"].rank(ascending=False)\n",
    "    df_norm.loc[group.index,\"Ranking_PLT2\"]=df_norm.loc[group.index,\"Z-PLT2\"].rank(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.to_csv(directorio_csv + \"/df_norm_\"+Target_name+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43829eb3",
   "metadata": {},
   "source": [
    "## Repetimos para el dataframe inverso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e71c39",
   "metadata": {},
   "source": [
    "Por si hay demasiados pocos en los filtrados añadir morralla, no son importantes solo para el submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_inv=pd.read_csv(directorio_csv+'/pydock4_all_filtered_inv.csv')\n",
    "\n",
    "# Removing unnecesary columns\n",
    "columnas=['Conf','RANK']\n",
    "#df_norm_inv.drop(columnas, axis=1, inplace=True)\n",
    "#df_norm_inv.dropna(subset=[\"Complex\"],inplace=True)\n",
    "\n",
    "# Removing duplicates\n",
    "df_norm_inv=df_norm_inv.drop_duplicates(subset=[\"Name\"],keep=\"first\")\n",
    "duplicados = df_norm_inv[df_norm_inv.duplicated(subset=[\"Name\",\"Version\",\"Complex\",\"Recycle\",\"State\"])]\n",
    "\n",
    "# Adding Total2 column\n",
    "df_norm_inv[\"Total2\"]=df_norm_inv[\"VDW\"]+df_norm_inv[\"Ele\"]+df_norm_inv[\"Desolv\"] \n",
    "\n",
    "# Z-Score individuales, inicializacion\n",
    "df_norm_inv[\"MCZ-Score\"] = 0 # Z-score de model_conficence\n",
    "df_norm_inv[\"PLDDTZ-Score\"] = 0 # Z-score de pLDDT\n",
    "df_norm_inv[\"TEZ-Score\"] = 0 # Z-score de Total\n",
    "df_norm_inv[\"TE2Z-Score\"] = 0 # Z-score de Total2\n",
    "\n",
    "# Suma de Z-Score, inicialicion\n",
    "df_norm_inv[\"Sum_Z\"] = 0 # Z-score Model confidence + Total\n",
    "df_norm_inv[\"Sum2_Z\"] = 0 # Z-score Model confidence + Total2\n",
    "df_norm_inv[\"Z-PLT\"] = 0 # Z-score de pLDDT + Total\n",
    "df_norm_inv[\"Z-PLT2\"]= 0 # Z-score de pLDDT + Total2\n",
    "\n",
    "# Ranking Z-Score, inicializacion\n",
    "df_norm_inv[\"Ranking_Z\"] = 0 # Ranking de Sum_Z\n",
    "df_norm_inv[\"Ranking2_Z\"] = 0 # Ranking de Sum2_Z\n",
    "df_norm_inv[\"Ranking_PLT\"] = 0 # Ranking de Z-PLT\n",
    "df_norm_inv[\"Ranking_PLT2\"] = 0 # Ranking de Z-PLT2\n",
    "\n",
    "# Calculo de medias y desviaciones segun complejo\n",
    "grouped = df_norm_inv.groupby([ \"Complex\"])\n",
    "medias=grouped.mean()\n",
    "sdesv=grouped.std()\n",
    "\n",
    "# Z-Score individuales\n",
    "for name, group in grouped:\n",
    "    # Calculamos Z_score de model_conficence y total energy\n",
    "    df_norm_inv.loc[group.index,[\"MCZ-Score\"]] = (group[\"Model_confidence\"]-medias.loc[name,\"Model_confidence\"])/sdesv.loc[name,\"Model_confidence\"]\n",
    "    df_norm_inv.loc[group.index,[\"TEZ-Score\"]] = (group[\"Total\"]-medias.loc[name,\"Total\"])/sdesv.loc[name,\"Total\"]\n",
    "    df_norm_inv.loc[group.index,[\"TE2Z-Score\"]] = (group[\"Total2\"]-medias.loc[name,\"Total2\"])/sdesv.loc[name,\"Total2\"]\n",
    "    df_norm_inv.loc[group.index,[\"PLDDTZ-Score\"]] = (group[\"pLDDT\"]-medias.loc[name,\"pLDDT\"])/sdesv.loc[name,\"pLDDT\"]\n",
    "\n",
    "# Suma de Z-Score\n",
    "df_norm_inv.loc[:,\"Sum_Z\"]=df_norm_inv.loc[:,\"MCZ-Score\"]-df_norm_inv.loc[:,\"TEZ-Score\"]\n",
    "df_norm_inv.loc[:,\"Sum2_Z\"]=df_norm_inv.loc[:,\"MCZ-Score\"]-df_norm_inv.loc[:,\"TE2Z-Score\"]\n",
    "df_norm_inv.loc[:,\"Z-PLT\"]=df_norm_inv.loc[:,\"PLDDTZ-Score\"]-df_norm_inv.loc[:,\"TEZ-Score\"]\n",
    "df_norm_inv.loc[:,\"Z-PLT2\"]=df_norm_inv.loc[:,\"PLDDTZ-Score\"]-df_norm_inv.loc[:,\"TE2Z-Score\"]\n",
    "\n",
    "# Ranking Z-Score\n",
    "for name, group in grouped:\n",
    "    df_norm_inv.loc[group.index,\"Ranking_Z\"]=df_norm_inv.loc[group.index,\"Sum_Z\"].rank(ascending=False)\n",
    "    df_norm_inv.loc[group.index,\"Ranking2_Z\"]=df_norm_inv.loc[group.index,\"Sum2_Z\"].rank(ascending=False)\n",
    "    df_norm_inv.loc[group.index,\"Ranking_PLT\"]=df_norm_inv.loc[group.index,\"Z-PLT\"].rank(ascending=False)\n",
    "    df_norm_inv.loc[group.index,\"Ranking_PLT2\"]=df_norm_inv.loc[group.index,\"Z-PLT2\"].rank(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_inv.to_csv(directorio_csv + \"/df_norm_inv_\"+Target_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b31fb",
   "metadata": {},
   "source": [
    "### TOP100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198fcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm=pd.read_csv(directorio_csv + \"/df_norm_\"+Target_name+\".csv\")\n",
    "df_norm_inv=pd.read_csv(directorio_csv + \"/df_norm_inv_\"+Target_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def obtener_archivos_csv(directorio_csv):\n",
    "    archivos_csv = [archivo for archivo in os.listdir(directorio_csv) if archivo.startswith('df_norm')]\n",
    "    return archivos_csv\n",
    "\n",
    "# Reemplaza 'ruta/del/directorio' con la ruta real de tu directorio\n",
    "archivos_csv = obtener_archivos_csv(directorio_csv)\n",
    "\n",
    "if archivos_csv:\n",
    "    for archivo_csv in archivos_csv:\n",
    "        print(archivo_csv)\n",
    "else:\n",
    "    print(\"No se encontraron archivos CSV en el directorio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de835d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ranking = [ \"Ranking2_Z\"]\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    a = pd.read_csv(directorio + archivo)\n",
    "    if len(archivo.split(\"_\")) > 3: \n",
    "\n",
    "        inv='_'+archivo.split(\"_\")[2]\n",
    "        print(inv)\n",
    "    else:\n",
    "        inv=''\n",
    "\n",
    "    for complejo in a[\"Complex\"].unique():\n",
    "        df_complejo = a[a[\"Complex\"] == complejo].copy()\n",
    "        \n",
    "        for Rank in Ranking:\n",
    "            if \"PLT\" in Rank:\n",
    "               df_filtrado = df_complejo[df_complejo[\"Version\"] == \"deepfold_v1\"].copy()\n",
    "     \n",
    "            else:\n",
    "                df_filtrado = df_complejo\n",
    "                #print(df_filtrado)\n",
    "            \n",
    "            # Filtrar primero por los top 100 según Rank\n",
    "            top100_preorden = df_filtrado.nsmallest(100, Rank)\n",
    "            \n",
    "            # Luego, ordenar por Rank si es necesario\n",
    "            top100_ordenado = top100_preorden.sort_values(by=Rank, ascending=True)\n",
    "             # Crear la carpeta si no existe\n",
    "            print (directorio , complejo , \"_\" ,Rank ,inv)\n",
    "            nueva_carpeta = directorio + complejo + \"_\" + Rank + inv\n",
    "            os.makedirs(nueva_carpeta, exist_ok=True)\n",
    "            \n",
    "            # Mover archivos especificados en la columna 'PATH'\n",
    "            for idx, fila in top100_ordenado.iterrows():\n",
    "                ruta_original = fila['PATH']\n",
    "                shutil.copy(ruta_original, nueva_carpeta)\n",
    "            \n",
    "            # Guardar a CSV\n",
    "            print(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.txt\")\n",
    "            top100_ordenado['Name'].to_csv(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.txt\", index=False, header=False)\n",
    "            top100_ordenado.to_csv(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.csv\", index=False)\n",
    "            \n",
    "        #print(complejo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcbabc",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5037a7",
   "metadata": {},
   "source": [
    "Interesante para ver diferentes conformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2fa082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_archivo_ini(modelslist, RMSD_cutoff, receptor_mol, ligand_mol, filename='pyCluster_config.ini'):\n",
    "    import configparser\n",
    "    directorio = os.path.dirname(modelslist)\n",
    "    nombre_config = os.path.join(directorio, filename)\n",
    "    modelslist=os.path.basename(modelslist)\n",
    "    # Crear el objeto ConfigParser\n",
    "    config = configparser.ConfigParser()\n",
    "    # Agregar la sección 'clustering'\n",
    "    config['clustering'] = {\n",
    "        'modelslist': modelslist,\n",
    "        'RMSD_cutoff': RMSD_cutoff\n",
    "    }\n",
    "    # Agregar la sección 'receptor'\n",
    "    config['receptor'] = {\n",
    "        'mol': receptor_mol\n",
    "    }\n",
    "    # Agregar la sección 'ligand'\n",
    "    config['ligand'] = {\n",
    "        'mol': ligand_mol\n",
    "    }\n",
    "    # Escribir el archivo de configuración\n",
    "    with open(nombre_config, 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "    return os.path.basename(nombre_config)\n",
    "#cluster_list_files = [directorio +\"/\"+ complejo+\"_\"+nombre+\"/\"+ complejo+\"_\"+nombre+ \"_top100.txt\" for nombre in Ranking]\n",
    "cluster_list_files = [directorio +\"/\"+ Target_name+\"_\"+nombre+\"/\"+ Target_name+\"_\"+nombre+ \"_top100.txt\" for nombre in Ranking]\n",
    "\n",
    "# for cluster_list_file in cluster_list_files:\n",
    "#      #Ejecutar pydock4 pyCluster\n",
    "#      INI_FILE = crear_archivo_ini(cluster_list_file, 2, receptor_mol,ligand_mol)\n",
    "#      #INI_FILE = crear_archivo_ini(cluster_list_file, 4, receptor_mol,ligand_mol) #T266\n",
    "#      DIR_NAME = os.path.dirname(cluster_list_file)\n",
    "#      print (DIR_NAME)\n",
    "#      #subprocess.call(\"pydock4 \"+INI_FILE.strip(\".ini\")+\" pyCluster\", cwd=DIR_NAME, shell=True)\n",
    "#      #Generar los csv para Ranking clusterizados\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e101067",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_ordenado =pd.read_csv(directorio + complejo + \"_\" + Rank + \"/\"+ complejo + \"_\" + Rank + \"_top100.csv\")\n",
    "# clustered_list_file= pd.read_csv(DIR_NAME +\"/cluster_pyCluster_config.list\", header=None)\n",
    "\n",
    "# directorio + complejo + \"_\" + Rank \n",
    "# clustered_list_file.columns=['Name']\n",
    "# cols=top100_ordenado.columns\n",
    "# clustered_all_pydock = clustered_list_file.merge(top100_ordenado, on= 'Name')\n",
    "# # Reordenar las columnas según las columnas de top100_ordenado\n",
    "# clustered_all_pydock = clustered_all_pydock.reindex(columns=top100_ordenado.columns)\n",
    "\n",
    "# # Guardar el resultado en un archivo CSV\n",
    "# clustered_all_pydock.to_csv(DIR_NAME + \"/\" + Target_name + \"_cluster_pyCluster_config.csv\", index=False)\n",
    "\n",
    "# # Calcular la diferencia y agregarla como una nueva columna\n",
    "# clustered_all_pydock['Diferencia_R2_Z'] = clustered_all_pydock['Ranking2_Z'].diff(periods=-1) * -1\n",
    "\n",
    "# # Convertir las columnas seleccionadas a tipo numérico y manejar errores\n",
    "# cols_to_convert = ['Ranking_Z', 'Ranking2_Z', 'Ranking_PLT', 'Ranking_PLT2', 'Diferencia_R2_Z']\n",
    "# clustered_all_pydock[cols_to_convert] = clustered_all_pydock[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "\n",
    "# # Lógica para elegir el conjunto de datos\n",
    "# elegir_top100 = False\n",
    "\n",
    "# # Condición 1: Si hay más de un dato en clustered_all_pydock\n",
    "# if len(clustered_all_pydock) > 1:\n",
    "#     # Condición 2: Si entre los primeros 5 elementos de Diferencia_R2_Z hay alguno mayor que 10\n",
    "#     if (clustered_all_pydock['Diferencia_R2_Z'].head(5) > 10).any():\n",
    "#         elegir_top100 = True\n",
    "# else:\n",
    "#     elegir_top100 = True\n",
    "\n",
    "# # Seleccionar el DataFrame basado en las condiciones\n",
    "# if elegir_top100:\n",
    "#     df_to_send = top100_ordenado\n",
    "# else:\n",
    "#      # Seleccionar los datos de top100_ordenado que no están en clustered_all_pydock\n",
    "#     inverse_selection = top100_ordenado[~top100_ordenado['Name'].isin(clustered_all_pydock['Name'])]\n",
    "    \n",
    "#     # Concatenar clustered_all_pydock con la selección inversa\n",
    "#     df_to_send = pd.concat([clustered_all_pydock, inverse_selection], ignore_index=True)\n",
    "#     cols_to_convert = ['Ranking_Z', 'Ranking2_Z', 'Ranking_PLT', 'Ranking_PLT2', 'Diferencia_R2_Z']\n",
    "#     df_to_send[cols_to_convert] = df_to_send[cols_to_convert].apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saltar_clus=True\n",
    "if saltar_clus:\n",
    "    df_to_send=df_norm\n",
    "\n",
    "nueva_carpeta = directorio + complejo + \"_\" + Rank + '_inv'\n",
    "result_df_inverted2= pd.read_csv(nueva_carpeta + \"/\"+ complejo + \"_\" + Rank + \"_top100.csv\")\n",
    "result_df_inverted2=result_df_inverted2.sort_values('Ranking_PLT2')\n",
    "if len(df_to_send)< 100: \n",
    "    df_to_send = pd.concat([df_to_send, result_df_inverted2], ignore_index=True)\n",
    "    df_to_send = df_to_send[:100]\n",
    "df_to_send"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39818ef9",
   "metadata": {},
   "source": [
    "### Copy to the To_send directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc58ee",
   "metadata": {},
   "source": [
    "- Scorers: to send minuscula\n",
    "- Predictors: to send en mayuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68caeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(to_send_dir,exist_ok=True)\n",
    "for file in df_to_send['PATH']:\n",
    "    shutil.copy(file,to_send_dir)\n",
    "df_to_send.to_csv(to_send_csv, index=False)\n",
    "df_to_send.to_csv(to_send_csv.replace('ene','csv'), index=False)\n",
    "df_to_send['Name'].to_csv(to_send_csv.replace('ene','txt'), index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d2988",
   "metadata": {},
   "source": [
    "### Extra target T264 T265\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fbcc3",
   "metadata": {},
   "source": [
    "Leemos un ficher con los RMSD calculados y selccionamos los modelos para cada target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfc1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "3500*7/112/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "8000*7*12/60/60/112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64052e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSD_clusters_selection=pd.read_csv(to_send_dir + \"/\"+ \"RMSD_selection\",sep=\" \")\n",
    "# RMSD_clusters_selection = RMSD_clusters_selection.drop(index=0)\n",
    "# RMSD_clusters_selection =RMSD_clusters_selection.drop(columns=\"Unnamed: 5\")\n",
    "# RMSD_clusters_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5dc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(to_send_dir+'/T265',exist_ok=True)\n",
    "# filtered_model_names_T265 = RMSD_clusters_selection[(RMSD_clusters_selection.iloc[:, 1] < 8) | (RMSD_clusters_selection.iloc[:, 3] < 8)][\"Model_name\"]+'.pdb'\n",
    "# filtered_model_names_T265=filtered_model_names_T265.to_frame()\n",
    "# filtered_model_names_T265.to_csv(to_send_dir+'/T265'+'/T265_predictor_to_send.txt',index=False,header=None)\n",
    "# for file in filtered_model_names_T265['Model_name']:\n",
    "#     shutil.copy(to_send_dir+file,to_send_dir+'/T265')\n",
    "\n",
    "# os.makedirs(to_send_dir+'/T264',exist_ok=True)\n",
    "# filtered_model_names_T264 = RMSD_clusters_selection[(RMSD_clusters_selection.iloc[:, 2] < 8) | (RMSD_clusters_selection.iloc[:, 4] < 8)][\"Model_name\"]+'.pdb'\n",
    "# filtered_model_names_T264=filtered_model_names_T264.to_frame()\n",
    "# filtered_model_names_T264.to_csv(to_send_dir+'/T264'+'/T264_predictor_to_send.txt',index=False,header=None)\n",
    "# for file in filtered_model_names_T264['Model_name']:\n",
    "#     shutil.copy(to_send_dir+file,to_send_dir+'/T264')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb01ca",
   "metadata": {},
   "source": [
    "fold_t288_model_3.pdb fold_t288_model_0.pdb fold_t288_model_1.pdb T288_unrelaxed_rank_003_alphafold2_multimer_v2_model_1_seed_000.r19.pdb T288_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_001.r20.pdb T288_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_001.r19.pdb T288_unrelaxed_rank_009_alphafold2_multimer_v3_model_3_seed_000.r19.pdb T288_unrelaxed_rank_009_alphafold2_multimer_v3_model_3_seed_000.r18.pdb T288_unrelaxed_rank_009_alphafold2_multimer_v3_model_3_seed_000.r17.pdb T288_relaxed_rank_001_alphafold2_multimer_v3_model_1_seed_001.pdb T288_unrelaxed_rank_008_alphafold2_multimer_v3_model_2_seed_001.r6.pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7acee",
   "metadata": {},
   "source": [
    "T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r16.pdb T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r12.pdb T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r14.pdb T292_unrelaxed_rank_002_alphafold2_multimer_v3_model_1_seed_001.r13.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r11.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r19.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r15.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r17.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r14.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r20.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r10.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r16.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r18.pdb T292_relaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.pdb T292_unrelaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.r16.pdb T292_relaxed_rank_004_alphafold2_multimer_v2_model_4_seed_000.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r20.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r13.pdb T292_unrelaxed_rank_001_alphafold2_multimer_v2_model_3_seed_000.r12.pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb74785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511abda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
